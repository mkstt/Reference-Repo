{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lightning-demo-seed.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mkstt/Reference-Repo/blob/master/Lightning_demo_seed.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GatZ6ZiXFzVh",
        "colab_type": "text"
      },
      "source": [
        "![image.png](data:image/png;base64,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)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "goRmGIRI5cfC",
        "colab_type": "text"
      },
      "source": [
        "# Live DEMO\n",
        "\n",
        "This is a minimal working example of PyTorch Lightning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jKj5lgdr5j48",
        "colab_type": "text"
      },
      "source": [
        "### Setup  \n",
        "Lightning is easy to use. Simply ```pip install pytorch-lightning```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UGjilEHk4vb7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! pip install pytorch-lightning"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gjo55nA549pU",
        "colab_type": "text"
      },
      "source": [
        "### 1. LightningModule\n",
        "Each project goes into a LightningModule.\n",
        "This module houses:\n",
        "1. Model definition (__init__)\n",
        "2. Computations (forward)\n",
        "3. What happens inside the training loop (training_step)\n",
        "4. What happens inside the validation loop (validation_step)\n",
        "5. What optimizer(s) to use (configure_optimizers)\n",
        "6. What data to use (train_dataloader, val_dataloader, test_dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x-34xKCI40yW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "\n",
        "import torch\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import MNIST\n",
        "from torchvision import transforms\n",
        "import pytorch_lightning as pl\n",
        "\n",
        "class MNISTModel(pl.LightningModule):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(MNISTModel, self).__init__()\n",
        "        # not the best model...\n",
        "        self.l1 = torch.nn.Linear(28 * 28, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return torch.relu(self.l1(x.view(x.size(0), -1)))\n",
        "\n",
        "    def training_step(self, batch, batch_nb):\n",
        "        # REQUIRED\n",
        "        x, y = batch\n",
        "        y_hat = self.forward(x)\n",
        "        loss = F.cross_entropy(y_hat, y)\n",
        "        tensorboard_logs = {'train_loss': loss}\n",
        "        return {'loss': loss, 'log': tensorboard_logs}\n",
        "\n",
        "    def validation_step(self, batch, batch_nb):\n",
        "        # OPTIONAL\n",
        "        x, y = batch\n",
        "        y_hat = self.forward(x)\n",
        "        return {'val_loss': F.cross_entropy(y_hat, y)}\n",
        "\n",
        "    def validation_end(self, outputs):\n",
        "        # OPTIONAL\n",
        "        avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()\n",
        "        tensorboard_logs = {'val_loss': avg_loss}\n",
        "        return {'avg_val_loss': avg_loss, 'log': tensorboard_logs}\n",
        "\n",
        "    def test_step(self, batch, batch_nb):\n",
        "        # OPTIONAL\n",
        "        x, y = batch\n",
        "        y_hat = self.forward(x)\n",
        "        return {'test_loss': F.cross_entropy(y_hat, y)}\n",
        "\n",
        "    def test_end(self, outputs):\n",
        "        # OPTIONAL\n",
        "        avg_loss = torch.stack([x['test_loss'] for x in outputs]).mean()\n",
        "        logs = {'test_loss': avg_loss}\n",
        "        return {'avg_test_loss': avg_loss, 'log': logs, 'progress_bar': logs}\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        # REQUIRED\n",
        "        # can return multiple optimizers and learning_rate schedulers\n",
        "        # (LBFGS it is automatically supported, no need for closure function)\n",
        "        return torch.optim.Adam(self.parameters(), lr=0.02)\n",
        "\n",
        "    @pl.data_loader\n",
        "    def train_dataloader(self):\n",
        "        # REQUIRED\n",
        "        return DataLoader(MNIST(os.getcwd(), train=True, download=True, transform=transforms.ToTensor()), batch_size=32)\n",
        "\n",
        "    @pl.data_loader\n",
        "    def val_dataloader(self):\n",
        "        # OPTIONAL\n",
        "        return DataLoader(MNIST(os.getcwd(), train=True, download=True, transform=transforms.ToTensor()), batch_size=32)\n",
        "\n",
        "    @pl.data_loader\n",
        "    def test_dataloader(self):\n",
        "        # OPTIONAL\n",
        "        return DataLoader(MNIST(os.getcwd(), train=False, download=True, transform=transforms.ToTensor()), batch_size=32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GROo8IDX-QCx",
        "colab_type": "text"
      },
      "source": [
        "#### 2. Trainer\n",
        "The trainer is where the magic happens!\n",
        "Feed ANY LightningModule to a trainer to train the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HOk9c4_35FKg",
        "colab_type": "code",
        "outputId": "b178f6ca-374f-48ab-8aa7-9aa65d82c881",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "mnist_model = MNISTModel()\n",
        "\n",
        "# most basic trainer, uses good defaults (1 gpu)\n",
        "trainer = pl.Trainer(gpus=1)    \n",
        "trainer.fit(mnist_model)   "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:root:gpu available: True, used: True\n",
            "INFO:root:VISIBLE GPUS: 0\n",
            "Epoch 1:  81%|████████▏ | 3049/3750 [00:30<00:01, 369.02batch/s, batch_nb=1874, loss=0.652, v_nb=0]\n",
            "Validating:  63%|██████▎   | 1174/1875 [00:18<00:01, 377.73batch/s]\u001b[AINFO:root:  Name    Type Params\n",
            "0   l1  Linear    7 K\n",
            "Epoch 1:  50%|█████     | 1875/3750 [00:11<00:11, 158.39batch/s, batch_nb=1874, gpu=0, loss=0.931, v_nb=1]\n",
            "Epoch 1:  50%|█████     | 1878/3750 [00:11<00:11, 156.60batch/s, batch_nb=1874, gpu=0, loss=0.931, v_nb=1]\n",
            "Epoch 1:  51%|█████     | 1918/3750 [00:12<00:09, 191.00batch/s, batch_nb=1874, gpu=0, loss=0.931, v_nb=1]\n",
            "Epoch 1:  52%|█████▏    | 1959/3750 [00:12<00:07, 226.68batch/s, batch_nb=1874, gpu=0, loss=0.931, v_nb=1]\n",
            "Epoch 1:  53%|█████▎    | 1998/3750 [00:12<00:06, 258.20batch/s, batch_nb=1874, gpu=0, loss=0.931, v_nb=1]\n",
            "Epoch 1:  54%|█████▍    | 2037/3750 [00:12<00:05, 287.19batch/s, batch_nb=1874, gpu=0, loss=0.931, v_nb=1]\n",
            "Epoch 1:  55%|█████▌    | 2076/3750 [00:12<00:05, 311.30batch/s, batch_nb=1874, gpu=0, loss=0.931, v_nb=1]\n",
            "Epoch 1:  56%|█████▋    | 2117/3750 [00:12<00:04, 334.92batch/s, batch_nb=1874, gpu=0, loss=0.931, v_nb=1]\n",
            "Epoch 1:  58%|█████▊    | 2157/3750 [00:12<00:04, 351.97batch/s, batch_nb=1874, gpu=0, loss=0.931, v_nb=1]\n",
            "Epoch 1:  59%|█████▊    | 2197/3750 [00:12<00:04, 363.32batch/s, batch_nb=1874, gpu=0, loss=0.931, v_nb=1]\n",
            "Epoch 1:  60%|█████▉    | 2238/3750 [00:12<00:04, 376.02batch/s, batch_nb=1874, gpu=0, loss=0.931, v_nb=1]\n",
            "Epoch 1:  61%|██████    | 2277/3750 [00:12<00:04, 358.48batch/s, batch_nb=1874, gpu=0, loss=0.931, v_nb=1]\n",
            "Epoch 1:  62%|██████▏   | 2317/3750 [00:13<00:03, 367.96batch/s, batch_nb=1874, gpu=0, loss=0.931, v_nb=1]\n",
            "Epoch 1:  63%|██████▎   | 2359/3750 [00:13<00:03, 380.72batch/s, batch_nb=1874, gpu=0, loss=0.931, v_nb=1]\n",
            "Epoch 1:  64%|██████▍   | 2400/3750 [00:13<00:03, 387.36batch/s, batch_nb=1874, gpu=0, loss=0.931, v_nb=1]\n",
            "Epoch 1:  65%|██████▌   | 2440/3750 [00:13<00:03, 384.53batch/s, batch_nb=1874, gpu=0, loss=0.931, v_nb=1]\n",
            "Epoch 1:  66%|██████▌   | 2479/3750 [00:13<00:03, 378.63batch/s, batch_nb=1874, gpu=0, loss=0.931, v_nb=1]\n",
            "Epoch 1:  67%|██████▋   | 2520/3750 [00:13<00:03, 385.52batch/s, batch_nb=1874, gpu=0, loss=0.931, v_nb=1]\n",
            "Epoch 1:  68%|██████▊   | 2559/3750 [00:13<00:03, 385.92batch/s, batch_nb=1874, gpu=0, loss=0.931, v_nb=1]\n",
            "Epoch 1:  69%|██████▉   | 2600/3750 [00:13<00:02, 392.76batch/s, batch_nb=1874, gpu=0, loss=0.931, v_nb=1]\n",
            "Epoch 1:  70%|███████   | 2642/3750 [00:13<00:02, 397.97batch/s, batch_nb=1874, gpu=0, loss=0.931, v_nb=1]\n",
            "Epoch 1:  72%|███████▏  | 2682/3750 [00:14<00:02, 388.06batch/s, batch_nb=1874, gpu=0, loss=0.931, v_nb=1]\n",
            "Epoch 1:  73%|███████▎  | 2723/3750 [00:14<00:02, 392.26batch/s, batch_nb=1874, gpu=0, loss=0.931, v_nb=1]\n",
            "Epoch 1:  74%|███████▎  | 2764/3750 [00:14<00:02, 396.16batch/s, batch_nb=1874, gpu=0, loss=0.931, v_nb=1]\n",
            "Epoch 1:  75%|███████▍  | 2804/3750 [00:14<00:02, 395.17batch/s, batch_nb=1874, gpu=0, loss=0.931, v_nb=1]\n",
            "Epoch 1:  76%|███████▌  | 2844/3750 [00:14<00:02, 395.27batch/s, batch_nb=1874, gpu=0, loss=0.931, v_nb=1]\n",
            "Epoch 1:  77%|███████▋  | 2885/3750 [00:14<00:02, 398.43batch/s, batch_nb=1874, gpu=0, loss=0.931, v_nb=1]\n",
            "Epoch 1:  78%|███████▊  | 2926/3750 [00:14<00:02, 399.32batch/s, batch_nb=1874, gpu=0, loss=0.931, v_nb=1]\n",
            "Epoch 1:  79%|███████▉  | 2967/3750 [00:14<00:01, 399.77batch/s, batch_nb=1874, gpu=0, loss=0.931, v_nb=1]\n",
            "Epoch 1:  80%|████████  | 3007/3750 [00:14<00:01, 393.97batch/s, batch_nb=1874, gpu=0, loss=0.931, v_nb=1]\n",
            "Epoch 1:  81%|████████▏ | 3049/3750 [00:14<00:01, 398.95batch/s, batch_nb=1874, gpu=0, loss=0.931, v_nb=1]\n",
            "Epoch 1:  82%|████████▏ | 3089/3750 [00:15<00:01, 392.61batch/s, batch_nb=1874, gpu=0, loss=0.931, v_nb=1]\n",
            "Epoch 1:  83%|████████▎ | 3129/3750 [00:15<00:01, 393.95batch/s, batch_nb=1874, gpu=0, loss=0.931, v_nb=1]\n",
            "Epoch 1:  85%|████████▍ | 3169/3750 [00:15<00:01, 393.99batch/s, batch_nb=1874, gpu=0, loss=0.931, v_nb=1]\n",
            "Epoch 1:  86%|████████▌ | 3210/3750 [00:15<00:01, 396.17batch/s, batch_nb=1874, gpu=0, loss=0.931, v_nb=1]\n",
            "Epoch 1:  87%|████████▋ | 3250/3750 [00:15<00:01, 394.71batch/s, batch_nb=1874, gpu=0, loss=0.931, v_nb=1]\n",
            "Epoch 1:  88%|████████▊ | 3291/3750 [00:15<00:01, 397.32batch/s, batch_nb=1874, gpu=0, loss=0.931, v_nb=1]\n",
            "Epoch 1:  89%|████████▉ | 3331/3750 [00:15<00:01, 397.94batch/s, batch_nb=1874, gpu=0, loss=0.931, v_nb=1]\n",
            "Epoch 1:  90%|████████▉ | 3371/3750 [00:15<00:00, 398.05batch/s, batch_nb=1874, gpu=0, loss=0.931, v_nb=1]\n",
            "Epoch 1:  91%|█████████ | 3413/3750 [00:15<00:00, 402.01batch/s, batch_nb=1874, gpu=0, loss=0.931, v_nb=1]\n",
            "Epoch 1:  92%|█████████▏| 3454/3750 [00:15<00:00, 403.53batch/s, batch_nb=1874, gpu=0, loss=0.931, v_nb=1]\n",
            "Epoch 1:  93%|█████████▎| 3495/3750 [00:16<00:00, 395.32batch/s, batch_nb=1874, gpu=0, loss=0.931, v_nb=1]\n",
            "Epoch 1:  94%|█████████▍| 3535/3750 [00:16<00:00, 392.73batch/s, batch_nb=1874, gpu=0, loss=0.931, v_nb=1]\n",
            "Epoch 1:  95%|█████████▌| 3575/3750 [00:16<00:00, 393.79batch/s, batch_nb=1874, gpu=0, loss=0.931, v_nb=1]\n",
            "Epoch 1:  96%|█████████▋| 3617/3750 [00:16<00:00, 400.75batch/s, batch_nb=1874, gpu=0, loss=0.931, v_nb=1]\n",
            "Epoch 1:  98%|█████████▊| 3658/3750 [00:16<00:00, 402.69batch/s, batch_nb=1874, gpu=0, loss=0.931, v_nb=1]\n",
            "Epoch 1:  99%|█████████▊| 3699/3750 [00:16<00:00, 402.48batch/s, batch_nb=1874, gpu=0, loss=0.931, v_nb=1]\n",
            "Epoch 1: 100%|██████████| 3750/3750 [00:16<00:00, 401.02batch/s, batch_nb=1874, gpu=0, loss=0.931, v_nb=1]\n",
            "                                                                   \u001b[A/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/trainer_io.py:210: UserWarning: Did not find hyperparameters at model.hparams. Saving checkpoint without hyperparameters\n",
            "  \"Did not find hyperparameters at model.hparams. Saving checkpoint without\"\n",
            "Epoch 2:  50%|█████     | 1875/3750 [00:11<00:12, 153.65batch/s, batch_nb=1874, gpu=0, loss=1.096, v_nb=1]\n",
            "Epoch 2:  51%|█████     | 1911/3750 [00:11<00:09, 185.04batch/s, batch_nb=1874, gpu=0, loss=1.096, v_nb=1]\n",
            "Epoch 2:  52%|█████▏    | 1949/3750 [00:11<00:08, 218.66batch/s, batch_nb=1874, gpu=0, loss=1.096, v_nb=1]\n",
            "Epoch 2:  53%|█████▎    | 1989/3750 [00:11<00:06, 252.52batch/s, batch_nb=1874, gpu=0, loss=1.096, v_nb=1]\n",
            "Epoch 2:  54%|█████▍    | 2029/3750 [00:12<00:06, 283.21batch/s, batch_nb=1874, gpu=0, loss=1.096, v_nb=1]\n",
            "Epoch 2:  55%|█████▌    | 2068/3750 [00:12<00:05, 307.03batch/s, batch_nb=1874, gpu=0, loss=1.096, v_nb=1]\n",
            "Epoch 2:  56%|█████▌    | 2107/3750 [00:12<00:05, 326.60batch/s, batch_nb=1874, gpu=0, loss=1.096, v_nb=1]\n",
            "Epoch 2:  57%|█████▋    | 2147/3750 [00:12<00:04, 343.97batch/s, batch_nb=1874, gpu=0, loss=1.096, v_nb=1]\n",
            "Epoch 2:  58%|█████▊    | 2187/3750 [00:12<00:04, 357.12batch/s, batch_nb=1874, gpu=0, loss=1.096, v_nb=1]\n",
            "Epoch 2:  59%|█████▉    | 2227/3750 [00:12<00:04, 366.93batch/s, batch_nb=1874, gpu=0, loss=1.096, v_nb=1]\n",
            "Epoch 2:  60%|██████    | 2266/3750 [00:12<00:04, 369.12batch/s, batch_nb=1874, gpu=0, loss=1.096, v_nb=1]\n",
            "Epoch 2:  61%|██████▏   | 2306/3750 [00:12<00:03, 377.81batch/s, batch_nb=1874, gpu=0, loss=1.096, v_nb=1]\n",
            "Epoch 2:  63%|██████▎   | 2345/3750 [00:12<00:03, 374.40batch/s, batch_nb=1874, gpu=0, loss=1.096, v_nb=1]\n",
            "Epoch 2:  64%|██████▎   | 2384/3750 [00:12<00:03, 378.65batch/s, batch_nb=1874, gpu=0, loss=1.096, v_nb=1]\n",
            "Epoch 2:  65%|██████▍   | 2424/3750 [00:13<00:03, 384.06batch/s, batch_nb=1874, gpu=0, loss=1.096, v_nb=1]\n",
            "Epoch 2:  66%|██████▌   | 2463/3750 [00:13<00:03, 375.83batch/s, batch_nb=1874, gpu=0, loss=1.096, v_nb=1]\n",
            "Epoch 2:  67%|██████▋   | 2501/3750 [00:13<00:03, 373.05batch/s, batch_nb=1874, gpu=0, loss=1.096, v_nb=1]\n",
            "Epoch 2:  68%|██████▊   | 2539/3750 [00:13<00:03, 368.19batch/s, batch_nb=1874, gpu=0, loss=1.096, v_nb=1]\n",
            "Epoch 2:  69%|██████▊   | 2576/3750 [00:13<00:03, 362.90batch/s, batch_nb=1874, gpu=0, loss=1.096, v_nb=1]\n",
            "Epoch 2:  70%|██████▉   | 2616/3750 [00:13<00:03, 372.29batch/s, batch_nb=1874, gpu=0, loss=1.096, v_nb=1]\n",
            "Epoch 2:  71%|███████   | 2654/3750 [00:13<00:02, 370.03batch/s, batch_nb=1874, gpu=0, loss=1.096, v_nb=1]\n",
            "Epoch 2:  72%|███████▏  | 2693/3750 [00:13<00:02, 373.75batch/s, batch_nb=1874, gpu=0, loss=1.096, v_nb=1]\n",
            "Epoch 2:  73%|███████▎  | 2731/3750 [00:13<00:02, 371.86batch/s, batch_nb=1874, gpu=0, loss=1.096, v_nb=1]\n",
            "Epoch 2:  74%|███████▍  | 2769/3750 [00:14<00:02, 373.68batch/s, batch_nb=1874, gpu=0, loss=1.096, v_nb=1]\n",
            "Epoch 2:  75%|███████▍  | 2808/3750 [00:14<00:02, 376.98batch/s, batch_nb=1874, gpu=0, loss=1.096, v_nb=1]\n",
            "Epoch 2:  76%|███████▌  | 2847/3750 [00:14<00:02, 379.47batch/s, batch_nb=1874, gpu=0, loss=1.096, v_nb=1]\n",
            "Epoch 2:  77%|███████▋  | 2885/3750 [00:14<00:02, 374.86batch/s, batch_nb=1874, gpu=0, loss=1.096, v_nb=1]\n",
            "Epoch 2:  78%|███████▊  | 2923/3750 [00:14<00:02, 372.95batch/s, batch_nb=1874, gpu=0, loss=1.096, v_nb=1]\n",
            "Epoch 2:  79%|███████▉  | 2962/3750 [00:14<00:02, 377.41batch/s, batch_nb=1874, gpu=0, loss=1.096, v_nb=1]\n",
            "Epoch 2:  80%|████████  | 3001/3750 [00:14<00:01, 379.49batch/s, batch_nb=1874, gpu=0, loss=1.096, v_nb=1]\n",
            "Epoch 2:  81%|████████  | 3039/3750 [00:14<00:01, 371.83batch/s, batch_nb=1874, gpu=0, loss=1.096, v_nb=1]\n",
            "Epoch 2:  82%|████████▏ | 3078/3750 [00:14<00:01, 374.00batch/s, batch_nb=1874, gpu=0, loss=1.096, v_nb=1]\n",
            "Epoch 2:  83%|████████▎ | 3116/3750 [00:14<00:01, 359.14batch/s, batch_nb=1874, gpu=0, loss=1.096, v_nb=1]\n",
            "Epoch 2:  84%|████████▍ | 3153/3750 [00:15<00:01, 359.09batch/s, batch_nb=1874, gpu=0, loss=1.096, v_nb=1]\n",
            "Epoch 2:  85%|████████▌ | 3193/3750 [00:15<00:01, 368.71batch/s, batch_nb=1874, gpu=0, loss=1.096, v_nb=1]\n",
            "Epoch 2:  86%|████████▌ | 3231/3750 [00:15<00:01, 371.25batch/s, batch_nb=1874, gpu=0, loss=1.096, v_nb=1]\n",
            "Epoch 2:  87%|████████▋ | 3269/3750 [00:15<00:01, 370.57batch/s, batch_nb=1874, gpu=0, loss=1.096, v_nb=1]\n",
            "Epoch 2:  88%|████████▊ | 3309/3750 [00:15<00:01, 376.44batch/s, batch_nb=1874, gpu=0, loss=1.096, v_nb=1]\n",
            "Epoch 2:  89%|████████▉ | 3347/3750 [00:15<00:01, 375.93batch/s, batch_nb=1874, gpu=0, loss=1.096, v_nb=1]\n",
            "Epoch 2:  90%|█████████ | 3385/3750 [00:15<00:00, 375.68batch/s, batch_nb=1874, gpu=0, loss=1.096, v_nb=1]\n",
            "Epoch 2:  91%|█████████▏| 3423/3750 [00:15<00:00, 365.54batch/s, batch_nb=1874, gpu=0, loss=1.096, v_nb=1]\n",
            "Epoch 2:  92%|█████████▏| 3460/3750 [00:15<00:00, 364.95batch/s, batch_nb=1874, gpu=0, loss=1.096, v_nb=1]\n",
            "Epoch 2:  93%|█████████▎| 3498/3750 [00:15<00:00, 367.76batch/s, batch_nb=1874, gpu=0, loss=1.096, v_nb=1]\n",
            "Epoch 2:  94%|█████████▍| 3537/3750 [00:16<00:00, 373.59batch/s, batch_nb=1874, gpu=0, loss=1.096, v_nb=1]\n",
            "Epoch 2:  95%|█████████▌| 3576/3750 [00:16<00:00, 376.72batch/s, batch_nb=1874, gpu=0, loss=1.096, v_nb=1]\n",
            "Epoch 2:  96%|█████████▋| 3616/3750 [00:16<00:00, 381.33batch/s, batch_nb=1874, gpu=0, loss=1.096, v_nb=1]\n",
            "Epoch 2:  97%|█████████▋| 3655/3750 [00:16<00:00, 381.47batch/s, batch_nb=1874, gpu=0, loss=1.096, v_nb=1]\n",
            "Epoch 2:  99%|█████████▊| 3696/3750 [00:16<00:00, 387.54batch/s, batch_nb=1874, gpu=0, loss=1.096, v_nb=1]\n",
            "Epoch 2: 100%|█████████▉| 3735/3750 [00:16<00:00, 387.73batch/s, batch_nb=1874, gpu=0, loss=1.096, v_nb=1]\n",
            "Epoch 2: 100%|██████████| 3750/3750 [00:16<00:00, 387.73batch/s, batch_nb=1874, gpu=0, loss=1.096, v_nb=1]\n",
            "Epoch 3:  50%|█████     | 1875/3750 [00:12<00:12, 155.67batch/s, batch_nb=1874, gpu=0, loss=1.095, v_nb=1]\n",
            "Epoch 3:  50%|█████     | 1891/3750 [00:12<00:10, 174.63batch/s, batch_nb=1874, gpu=0, loss=1.095, v_nb=1]\n",
            "Epoch 3:  51%|█████▏    | 1924/3750 [00:12<00:08, 203.07batch/s, batch_nb=1874, gpu=0, loss=1.095, v_nb=1]\n",
            "Epoch 3:  52%|█████▏    | 1962/3750 [00:12<00:07, 235.31batch/s, batch_nb=1874, gpu=0, loss=1.095, v_nb=1]\n",
            "Epoch 3:  53%|█████▎    | 2001/3750 [00:12<00:06, 266.63batch/s, batch_nb=1874, gpu=0, loss=1.095, v_nb=1]\n",
            "Epoch 3:  54%|█████▍    | 2040/3750 [00:12<00:05, 294.06batch/s, batch_nb=1874, gpu=0, loss=1.095, v_nb=1]\n",
            "Epoch 3:  55%|█████▌    | 2080/3750 [00:12<00:05, 317.48batch/s, batch_nb=1874, gpu=0, loss=1.095, v_nb=1]\n",
            "Epoch 3:  57%|█████▋    | 2119/3750 [00:12<00:04, 335.15batch/s, batch_nb=1874, gpu=0, loss=1.095, v_nb=1]\n",
            "Epoch 3:  58%|█████▊    | 2158/3750 [00:13<00:04, 348.50batch/s, batch_nb=1874, gpu=0, loss=1.095, v_nb=1]\n",
            "Epoch 3:  59%|█████▊    | 2198/3750 [00:13<00:04, 362.37batch/s, batch_nb=1874, gpu=0, loss=1.095, v_nb=1]\n",
            "Epoch 3:  60%|█████▉    | 2236/3750 [00:13<00:04, 362.51batch/s, batch_nb=1874, gpu=0, loss=1.095, v_nb=1]\n",
            "Epoch 3:  61%|██████    | 2274/3750 [00:13<00:04, 362.74batch/s, batch_nb=1874, gpu=0, loss=1.095, v_nb=1]\n",
            "Epoch 3:  62%|██████▏   | 2311/3750 [00:13<00:04, 358.25batch/s, batch_nb=1874, gpu=0, loss=1.095, v_nb=1]\n",
            "Epoch 3:  63%|██████▎   | 2351/3750 [00:13<00:03, 367.49batch/s, batch_nb=1874, gpu=0, loss=1.095, v_nb=1]\n",
            "Epoch 3:  64%|██████▍   | 2391/3750 [00:13<00:03, 374.24batch/s, batch_nb=1874, gpu=0, loss=1.095, v_nb=1]\n",
            "Epoch 3:  65%|██████▍   | 2431/3750 [00:13<00:03, 379.39batch/s, batch_nb=1874, gpu=0, loss=1.095, v_nb=1]\n",
            "Epoch 3:  66%|██████▌   | 2470/3750 [00:13<00:03, 368.39batch/s, batch_nb=1874, gpu=0, loss=1.095, v_nb=1]\n",
            "Epoch 3:  67%|██████▋   | 2508/3750 [00:13<00:03, 371.67batch/s, batch_nb=1874, gpu=0, loss=1.095, v_nb=1]\n",
            "Epoch 3:  68%|██████▊   | 2548/3750 [00:14<00:03, 378.23batch/s, batch_nb=1874, gpu=0, loss=1.095, v_nb=1]\n",
            "Epoch 3:  69%|██████▉   | 2589/3750 [00:14<00:03, 384.80batch/s, batch_nb=1874, gpu=0, loss=1.095, v_nb=1]\n",
            "Epoch 3:  70%|███████   | 2628/3750 [00:14<00:02, 384.66batch/s, batch_nb=1874, gpu=0, loss=1.095, v_nb=1]\n",
            "Epoch 3:  71%|███████   | 2667/3750 [00:14<00:02, 385.75batch/s, batch_nb=1874, gpu=0, loss=1.095, v_nb=1]\n",
            "Epoch 3:  72%|███████▏  | 2706/3750 [00:14<00:02, 381.35batch/s, batch_nb=1874, gpu=0, loss=1.095, v_nb=1]\n",
            "Epoch 3:  73%|███████▎  | 2745/3750 [00:14<00:02, 380.50batch/s, batch_nb=1874, gpu=0, loss=1.095, v_nb=1]\n",
            "Epoch 3:  74%|███████▍  | 2784/3750 [00:14<00:02, 379.58batch/s, batch_nb=1874, gpu=0, loss=1.095, v_nb=1]\n",
            "Epoch 3:  75%|███████▌  | 2823/3750 [00:14<00:02, 381.06batch/s, batch_nb=1874, gpu=0, loss=1.095, v_nb=1]\n",
            "Epoch 3:  76%|███████▋  | 2862/3750 [00:14<00:02, 379.87batch/s, batch_nb=1874, gpu=0, loss=1.095, v_nb=1]\n",
            "Epoch 3:  77%|███████▋  | 2901/3750 [00:15<00:02, 380.45batch/s, batch_nb=1874, gpu=0, loss=1.095, v_nb=1]\n",
            "Epoch 3:  78%|███████▊  | 2940/3750 [00:15<00:02, 382.31batch/s, batch_nb=1874, gpu=0, loss=1.095, v_nb=1]\n",
            "Epoch 3:  79%|███████▉  | 2980/3750 [00:15<00:01, 386.89batch/s, batch_nb=1874, gpu=0, loss=1.095, v_nb=1]\n",
            "Epoch 3:  81%|████████  | 3020/3750 [00:15<00:01, 389.27batch/s, batch_nb=1874, gpu=0, loss=1.095, v_nb=1]\n",
            "Epoch 3:  82%|████████▏ | 3060/3750 [00:15<00:01, 391.19batch/s, batch_nb=1874, gpu=0, loss=1.095, v_nb=1]\n",
            "Epoch 3:  83%|████████▎ | 3100/3750 [00:15<00:01, 382.73batch/s, batch_nb=1874, gpu=0, loss=1.095, v_nb=1]\n",
            "Epoch 3:  84%|████████▎ | 3139/3750 [00:15<00:01, 378.61batch/s, batch_nb=1874, gpu=0, loss=1.095, v_nb=1]\n",
            "Epoch 3:  85%|████████▍ | 3178/3750 [00:15<00:01, 381.74batch/s, batch_nb=1874, gpu=0, loss=1.095, v_nb=1]\n",
            "Epoch 3:  86%|████████▌ | 3218/3750 [00:15<00:01, 385.00batch/s, batch_nb=1874, gpu=0, loss=1.095, v_nb=1]\n",
            "Epoch 3:  87%|████████▋ | 3257/3750 [00:15<00:01, 367.09batch/s, batch_nb=1874, gpu=0, loss=1.095, v_nb=1]\n",
            "Epoch 3:  88%|████████▊ | 3294/3750 [00:16<00:01, 362.68batch/s, batch_nb=1874, gpu=0, loss=1.095, v_nb=1]\n",
            "Epoch 3:  89%|████████▉ | 3334/3750 [00:16<00:01, 371.63batch/s, batch_nb=1874, gpu=0, loss=1.095, v_nb=1]\n",
            "Epoch 3:  90%|█████████ | 3375/3750 [00:16<00:00, 380.57batch/s, batch_nb=1874, gpu=0, loss=1.095, v_nb=1]\n",
            "Epoch 3:  91%|█████████ | 3415/3750 [00:16<00:00, 385.07batch/s, batch_nb=1874, gpu=0, loss=1.095, v_nb=1]\n",
            "Epoch 3:  92%|█████████▏| 3454/3750 [00:16<00:00, 382.16batch/s, batch_nb=1874, gpu=0, loss=1.095, v_nb=1]\n",
            "Epoch 3:  93%|█████████▎| 3493/3750 [00:16<00:00, 372.77batch/s, batch_nb=1874, gpu=0, loss=1.095, v_nb=1]\n",
            "Epoch 3:  94%|█████████▍| 3531/3750 [00:16<00:00, 365.82batch/s, batch_nb=1874, gpu=0, loss=1.095, v_nb=1]\n",
            "Epoch 3:  95%|█████████▌| 3569/3750 [00:16<00:00, 369.06batch/s, batch_nb=1874, gpu=0, loss=1.095, v_nb=1]\n",
            "Epoch 3:  96%|█████████▌| 3607/3750 [00:16<00:00, 371.08batch/s, batch_nb=1874, gpu=0, loss=1.095, v_nb=1]\n",
            "Epoch 3:  97%|█████████▋| 3645/3750 [00:16<00:00, 371.96batch/s, batch_nb=1874, gpu=0, loss=1.095, v_nb=1]\n",
            "Epoch 3:  98%|█████████▊| 3683/3750 [00:17<00:00, 368.15batch/s, batch_nb=1874, gpu=0, loss=1.095, v_nb=1]\n",
            "Epoch 3:  99%|█████████▉| 3723/3750 [00:17<00:00, 374.90batch/s, batch_nb=1874, gpu=0, loss=1.095, v_nb=1]\n",
            "Epoch 3: 100%|██████████| 3750/3750 [00:17<00:00, 374.90batch/s, batch_nb=1874, gpu=0, loss=1.095, v_nb=1]\n",
            "Epoch 4:  50%|█████     | 1875/3750 [00:12<00:12, 147.06batch/s, batch_nb=1874, gpu=0, loss=1.092, v_nb=1]\n",
            "Epoch 4:  50%|█████     | 1893/3750 [00:12<00:11, 166.08batch/s, batch_nb=1874, gpu=0, loss=1.092, v_nb=1]\n",
            "Epoch 4:  51%|█████▏    | 1930/3750 [00:12<00:09, 198.10batch/s, batch_nb=1874, gpu=0, loss=1.092, v_nb=1]\n",
            "Epoch 4:  52%|█████▏    | 1965/3750 [00:12<00:07, 227.50batch/s, batch_nb=1874, gpu=0, loss=1.092, v_nb=1]\n",
            "Epoch 4:  53%|█████▎    | 2004/3750 [00:12<00:06, 259.24batch/s, batch_nb=1874, gpu=0, loss=1.092, v_nb=1]\n",
            "Epoch 4:  54%|█████▍    | 2042/3750 [00:12<00:05, 285.42batch/s, batch_nb=1874, gpu=0, loss=1.092, v_nb=1]\n",
            "Epoch 4:  55%|█████▌    | 2081/3750 [00:12<00:05, 309.82batch/s, batch_nb=1874, gpu=0, loss=1.092, v_nb=1]\n",
            "Epoch 4:  57%|█████▋    | 2121/3750 [00:13<00:04, 330.87batch/s, batch_nb=1874, gpu=0, loss=1.092, v_nb=1]\n",
            "Epoch 4:  58%|█████▊    | 2161/3750 [00:13<00:04, 347.36batch/s, batch_nb=1874, gpu=0, loss=1.092, v_nb=1]\n",
            "Epoch 4:  59%|█████▊    | 2199/3750 [00:13<00:04, 355.19batch/s, batch_nb=1874, gpu=0, loss=1.092, v_nb=1]\n",
            "Epoch 4:  60%|█████▉    | 2238/3750 [00:13<00:04, 362.74batch/s, batch_nb=1874, gpu=0, loss=1.092, v_nb=1]\n",
            "Epoch 4:  61%|██████    | 2276/3750 [00:13<00:04, 363.21batch/s, batch_nb=1874, gpu=0, loss=1.092, v_nb=1]\n",
            "Epoch 4:  62%|██████▏   | 2316/3750 [00:13<00:03, 371.17batch/s, batch_nb=1874, gpu=0, loss=1.092, v_nb=1]\n",
            "Epoch 4:  63%|██████▎   | 2354/3750 [00:13<00:03, 364.05batch/s, batch_nb=1874, gpu=0, loss=1.092, v_nb=1]\n",
            "Epoch 4:  64%|██████▍   | 2391/3750 [00:13<00:03, 362.23batch/s, batch_nb=1874, gpu=0, loss=1.092, v_nb=1]\n",
            "Epoch 4:  65%|██████▍   | 2428/3750 [00:13<00:03, 359.98batch/s, batch_nb=1874, gpu=0, loss=1.092, v_nb=1]\n",
            "Epoch 4:  66%|██████▌   | 2467/3750 [00:13<00:03, 367.80batch/s, batch_nb=1874, gpu=0, loss=1.092, v_nb=1]\n",
            "Epoch 4:  67%|██████▋   | 2506/3750 [00:14<00:03, 372.42batch/s, batch_nb=1874, gpu=0, loss=1.092, v_nb=1]\n",
            "Epoch 4:  68%|██████▊   | 2544/3750 [00:14<00:03, 358.05batch/s, batch_nb=1874, gpu=0, loss=1.092, v_nb=1]\n",
            "Epoch 4:  69%|██████▉   | 2582/3750 [00:14<00:03, 363.49batch/s, batch_nb=1874, gpu=0, loss=1.092, v_nb=1]\n",
            "Epoch 4:  70%|██████▉   | 2619/3750 [00:14<00:03, 364.86batch/s, batch_nb=1874, gpu=0, loss=1.092, v_nb=1]\n",
            "Epoch 4:  71%|███████   | 2657/3750 [00:14<00:02, 368.13batch/s, batch_nb=1874, gpu=0, loss=1.092, v_nb=1]\n",
            "Epoch 4:  72%|███████▏  | 2694/3750 [00:14<00:02, 368.07batch/s, batch_nb=1874, gpu=0, loss=1.092, v_nb=1]\n",
            "Epoch 4:  73%|███████▎  | 2732/3750 [00:14<00:02, 369.32batch/s, batch_nb=1874, gpu=0, loss=1.092, v_nb=1]\n",
            "Epoch 4:  74%|███████▍  | 2771/3750 [00:14<00:02, 373.87batch/s, batch_nb=1874, gpu=0, loss=1.092, v_nb=1]\n",
            "Epoch 4:  75%|███████▍  | 2809/3750 [00:14<00:02, 375.40batch/s, batch_nb=1874, gpu=0, loss=1.092, v_nb=1]\n",
            "Epoch 4:  76%|███████▌  | 2847/3750 [00:14<00:02, 375.09batch/s, batch_nb=1874, gpu=0, loss=1.092, v_nb=1]\n",
            "Epoch 4:  77%|███████▋  | 2885/3750 [00:15<00:02, 375.97batch/s, batch_nb=1874, gpu=0, loss=1.092, v_nb=1]\n",
            "Epoch 4:  78%|███████▊  | 2924/3750 [00:15<00:02, 378.66batch/s, batch_nb=1874, gpu=0, loss=1.092, v_nb=1]\n",
            "Epoch 4:  79%|███████▉  | 2963/3750 [00:15<00:02, 379.44batch/s, batch_nb=1874, gpu=0, loss=1.092, v_nb=1]\n",
            "Epoch 4:  80%|████████  | 3001/3750 [00:15<00:02, 373.83batch/s, batch_nb=1874, gpu=0, loss=1.092, v_nb=1]\n",
            "Epoch 4:  81%|████████  | 3040/3750 [00:15<00:01, 377.32batch/s, batch_nb=1874, gpu=0, loss=1.092, v_nb=1]\n",
            "Epoch 4:  82%|████████▏ | 3078/3750 [00:15<00:01, 375.57batch/s, batch_nb=1874, gpu=0, loss=1.092, v_nb=1]\n",
            "Epoch 4:  83%|████████▎ | 3116/3750 [00:15<00:01, 374.48batch/s, batch_nb=1874, gpu=0, loss=1.092, v_nb=1]\n",
            "Epoch 4:  84%|████████▍ | 3155/3750 [00:15<00:01, 378.76batch/s, batch_nb=1874, gpu=0, loss=1.092, v_nb=1]\n",
            "Epoch 4:  85%|████████▌ | 3194/3750 [00:15<00:01, 380.29batch/s, batch_nb=1874, gpu=0, loss=1.092, v_nb=1]\n",
            "Epoch 4:  86%|████████▌ | 3233/3750 [00:15<00:01, 373.86batch/s, batch_nb=1874, gpu=0, loss=1.092, v_nb=1]\n",
            "Epoch 4:  87%|████████▋ | 3271/3750 [00:16<00:01, 375.50batch/s, batch_nb=1874, gpu=0, loss=1.092, v_nb=1]\n",
            "Epoch 4:  88%|████████▊ | 3311/3750 [00:16<00:01, 380.12batch/s, batch_nb=1874, gpu=0, loss=1.092, v_nb=1]\n",
            "Epoch 4:  89%|████████▉ | 3351/3750 [00:16<00:01, 385.58batch/s, batch_nb=1874, gpu=0, loss=1.092, v_nb=1]\n",
            "Epoch 4:  90%|█████████ | 3390/3750 [00:16<00:00, 383.27batch/s, batch_nb=1874, gpu=0, loss=1.092, v_nb=1]\n",
            "Epoch 4:  91%|█████████▏| 3429/3750 [00:16<00:00, 385.20batch/s, batch_nb=1874, gpu=0, loss=1.092, v_nb=1]\n",
            "Epoch 4:  92%|█████████▏| 3468/3750 [00:16<00:00, 383.20batch/s, batch_nb=1874, gpu=0, loss=1.092, v_nb=1]\n",
            "Epoch 4:  94%|█████████▎| 3508/3750 [00:16<00:00, 386.21batch/s, batch_nb=1874, gpu=0, loss=1.092, v_nb=1]\n",
            "Epoch 4:  95%|█████████▍| 3548/3750 [00:16<00:00, 387.80batch/s, batch_nb=1874, gpu=0, loss=1.092, v_nb=1]\n",
            "Epoch 4:  96%|█████████▌| 3589/3750 [00:16<00:00, 392.70batch/s, batch_nb=1874, gpu=0, loss=1.092, v_nb=1]\n",
            "Epoch 4:  97%|█████████▋| 3629/3750 [00:17<00:00, 393.60batch/s, batch_nb=1874, gpu=0, loss=1.092, v_nb=1]\n",
            "Epoch 4:  98%|█████████▊| 3669/3750 [00:17<00:00, 393.92batch/s, batch_nb=1874, gpu=0, loss=1.092, v_nb=1]\n",
            "Epoch 4:  99%|█████████▉| 3709/3750 [00:17<00:00, 384.79batch/s, batch_nb=1874, gpu=0, loss=1.092, v_nb=1]\n",
            "Epoch 4: 100%|██████████| 3750/3750 [00:17<00:00, 377.06batch/s, batch_nb=1874, gpu=0, loss=1.092, v_nb=1]\n",
            "Epoch 5:  50%|█████     | 1875/3750 [00:12<00:12, 147.02batch/s, batch_nb=1874, gpu=0, loss=1.080, v_nb=1]\n",
            "Epoch 5:  50%|█████     | 1876/3750 [00:12<00:13, 143.09batch/s, batch_nb=1874, gpu=0, loss=1.080, v_nb=1]\n",
            "Epoch 5:  51%|█████     | 1913/3750 [00:12<00:10, 174.93batch/s, batch_nb=1874, gpu=0, loss=1.080, v_nb=1]\n",
            "Epoch 5:  52%|█████▏    | 1948/3750 [00:12<00:08, 205.73batch/s, batch_nb=1874, gpu=0, loss=1.080, v_nb=1]\n",
            "Epoch 5:  53%|█████▎    | 1988/3750 [00:12<00:07, 239.93batch/s, batch_nb=1874, gpu=0, loss=1.080, v_nb=1]\n",
            "Epoch 5:  54%|█████▍    | 2028/3750 [00:12<00:06, 270.99batch/s, batch_nb=1874, gpu=0, loss=1.080, v_nb=1]\n",
            "Epoch 5:  55%|█████▌    | 2067/3750 [00:12<00:05, 297.90batch/s, batch_nb=1874, gpu=0, loss=1.080, v_nb=1]\n",
            "Epoch 5:  56%|█████▌    | 2104/3750 [00:13<00:05, 316.21batch/s, batch_nb=1874, gpu=0, loss=1.080, v_nb=1]\n",
            "Epoch 5:  57%|█████▋    | 2142/3750 [00:13<00:04, 331.96batch/s, batch_nb=1874, gpu=0, loss=1.080, v_nb=1]\n",
            "Epoch 5:  58%|█████▊    | 2181/3750 [00:13<00:04, 346.26batch/s, batch_nb=1874, gpu=0, loss=1.080, v_nb=1]\n",
            "Epoch 5:  59%|█████▉    | 2221/3750 [00:13<00:04, 359.57batch/s, batch_nb=1874, gpu=0, loss=1.080, v_nb=1]\n",
            "Epoch 5:  60%|██████    | 2261/3750 [00:13<00:04, 368.52batch/s, batch_nb=1874, gpu=0, loss=1.080, v_nb=1]\n",
            "Epoch 5:  61%|██████▏   | 2301/3750 [00:13<00:03, 376.44batch/s, batch_nb=1874, gpu=0, loss=1.080, v_nb=1]\n",
            "Epoch 5:  62%|██████▏   | 2340/3750 [00:13<00:03, 370.48batch/s, batch_nb=1874, gpu=0, loss=1.080, v_nb=1]\n",
            "Epoch 5:  63%|██████▎   | 2380/3750 [00:13<00:03, 378.50batch/s, batch_nb=1874, gpu=0, loss=1.080, v_nb=1]\n",
            "Epoch 5:  65%|██████▍   | 2419/3750 [00:13<00:03, 378.17batch/s, batch_nb=1874, gpu=0, loss=1.080, v_nb=1]\n",
            "Epoch 5:  66%|██████▌   | 2459/3750 [00:13<00:03, 382.40batch/s, batch_nb=1874, gpu=0, loss=1.080, v_nb=1]\n",
            "Epoch 5:  67%|██████▋   | 2498/3750 [00:14<00:03, 380.08batch/s, batch_nb=1874, gpu=0, loss=1.080, v_nb=1]\n",
            "Epoch 5:  68%|██████▊   | 2537/3750 [00:14<00:03, 380.06batch/s, batch_nb=1874, gpu=0, loss=1.080, v_nb=1]\n",
            "Epoch 5:  69%|██████▊   | 2576/3750 [00:14<00:03, 379.80batch/s, batch_nb=1874, gpu=0, loss=1.080, v_nb=1]\n",
            "Epoch 5:  70%|██████▉   | 2617/3750 [00:14<00:02, 387.68batch/s, batch_nb=1874, gpu=0, loss=1.080, v_nb=1]\n",
            "Epoch 5:  71%|███████   | 2658/3750 [00:14<00:02, 390.48batch/s, batch_nb=1874, gpu=0, loss=1.080, v_nb=1]\n",
            "Epoch 5:  72%|███████▏  | 2698/3750 [00:14<00:02, 383.87batch/s, batch_nb=1874, gpu=0, loss=1.080, v_nb=1]\n",
            "Epoch 5:  73%|███████▎  | 2737/3750 [00:14<00:02, 374.59batch/s, batch_nb=1874, gpu=0, loss=1.080, v_nb=1]\n",
            "Epoch 5:  74%|███████▍  | 2776/3750 [00:14<00:02, 378.75batch/s, batch_nb=1874, gpu=0, loss=1.080, v_nb=1]\n",
            "Epoch 5:  75%|███████▌  | 2815/3750 [00:14<00:02, 379.38batch/s, batch_nb=1874, gpu=0, loss=1.080, v_nb=1]\n",
            "Epoch 5:  76%|███████▌  | 2855/3750 [00:14<00:02, 384.47batch/s, batch_nb=1874, gpu=0, loss=1.080, v_nb=1]\n",
            "Epoch 5:  77%|███████▋  | 2894/3750 [00:15<00:02, 384.37batch/s, batch_nb=1874, gpu=0, loss=1.080, v_nb=1]\n",
            "Epoch 5:  78%|███████▊  | 2934/3750 [00:15<00:02, 387.60batch/s, batch_nb=1874, gpu=0, loss=1.080, v_nb=1]\n",
            "Epoch 5:  79%|███████▉  | 2974/3750 [00:15<00:01, 388.97batch/s, batch_nb=1874, gpu=0, loss=1.080, v_nb=1]\n",
            "Epoch 5:  80%|████████  | 3013/3750 [00:15<00:01, 385.01batch/s, batch_nb=1874, gpu=0, loss=1.080, v_nb=1]\n",
            "Epoch 5:  81%|████████▏ | 3053/3750 [00:15<00:01, 387.72batch/s, batch_nb=1874, gpu=0, loss=1.080, v_nb=1]\n",
            "Epoch 5:  83%|████████▎ | 3094/3750 [00:15<00:01, 393.39batch/s, batch_nb=1874, gpu=0, loss=1.080, v_nb=1]\n",
            "Epoch 5:  84%|████████▎ | 3134/3750 [00:15<00:01, 377.55batch/s, batch_nb=1874, gpu=0, loss=1.080, v_nb=1]\n",
            "Epoch 5:  85%|████████▍ | 3173/3750 [00:15<00:01, 379.67batch/s, batch_nb=1874, gpu=0, loss=1.080, v_nb=1]\n",
            "Epoch 5:  86%|████████▌ | 3213/3750 [00:15<00:01, 382.94batch/s, batch_nb=1874, gpu=0, loss=1.080, v_nb=1]\n",
            "Epoch 5:  87%|████████▋ | 3253/3750 [00:16<00:01, 385.98batch/s, batch_nb=1874, gpu=0, loss=1.080, v_nb=1]\n",
            "Epoch 5:  88%|████████▊ | 3293/3750 [00:16<00:01, 390.04batch/s, batch_nb=1874, gpu=0, loss=1.080, v_nb=1]\n",
            "Epoch 5:  89%|████████▉ | 3333/3750 [00:16<00:01, 389.27batch/s, batch_nb=1874, gpu=0, loss=1.080, v_nb=1]\n",
            "Epoch 5:  90%|████████▉ | 3373/3750 [00:16<00:00, 390.35batch/s, batch_nb=1874, gpu=0, loss=1.080, v_nb=1]\n",
            "Epoch 5:  91%|█████████ | 3414/3750 [00:16<00:00, 394.78batch/s, batch_nb=1874, gpu=0, loss=1.080, v_nb=1]\n",
            "Epoch 5:  92%|█████████▏| 3455/3750 [00:16<00:00, 396.47batch/s, batch_nb=1874, gpu=0, loss=1.080, v_nb=1]\n",
            "Epoch 5:  93%|█████████▎| 3495/3750 [00:16<00:00, 394.17batch/s, batch_nb=1874, gpu=0, loss=1.080, v_nb=1]\n",
            "Epoch 5:  94%|█████████▍| 3535/3750 [00:16<00:00, 383.35batch/s, batch_nb=1874, gpu=0, loss=1.080, v_nb=1]\n",
            "Epoch 5:  95%|█████████▌| 3575/3750 [00:16<00:00, 386.57batch/s, batch_nb=1874, gpu=0, loss=1.080, v_nb=1]\n",
            "Epoch 5:  96%|█████████▋| 3616/3750 [00:16<00:00, 391.26batch/s, batch_nb=1874, gpu=0, loss=1.080, v_nb=1]\n",
            "Epoch 5:  97%|█████████▋| 3656/3750 [00:17<00:00, 389.61batch/s, batch_nb=1874, gpu=0, loss=1.080, v_nb=1]\n",
            "Epoch 5:  99%|█████████▊| 3696/3750 [00:17<00:00, 390.46batch/s, batch_nb=1874, gpu=0, loss=1.080, v_nb=1]\n",
            "Epoch 5: 100%|██████████| 3750/3750 [00:17<00:00, 382.53batch/s, batch_nb=1874, gpu=0, loss=1.080, v_nb=1]\n",
            "Epoch 6:  50%|█████     | 1875/3750 [00:12<00:12, 145.19batch/s, batch_nb=1874, gpu=0, loss=1.084, v_nb=1]\n",
            "Epoch 6:  51%|█████     | 1908/3750 [00:13<00:10, 176.34batch/s, batch_nb=1874, gpu=0, loss=1.084, v_nb=1]\n",
            "Epoch 6:  52%|█████▏    | 1946/3750 [00:13<00:08, 209.77batch/s, batch_nb=1874, gpu=0, loss=1.084, v_nb=1]\n",
            "Epoch 6:  53%|█████▎    | 1984/3750 [00:13<00:07, 241.78batch/s, batch_nb=1874, gpu=0, loss=1.084, v_nb=1]\n",
            "Epoch 6:  54%|█████▍    | 2024/3750 [00:13<00:06, 273.95batch/s, batch_nb=1874, gpu=0, loss=1.084, v_nb=1]\n",
            "Epoch 6:  55%|█████▍    | 2062/3750 [00:13<00:05, 297.68batch/s, batch_nb=1874, gpu=0, loss=1.084, v_nb=1]\n",
            "Epoch 6:  56%|█████▌    | 2100/3750 [00:13<00:05, 317.01batch/s, batch_nb=1874, gpu=0, loss=1.084, v_nb=1]\n",
            "Epoch 6:  57%|█████▋    | 2137/3750 [00:13<00:04, 330.97batch/s, batch_nb=1874, gpu=0, loss=1.084, v_nb=1]\n",
            "Epoch 6:  58%|█████▊    | 2173/3750 [00:13<00:04, 337.12batch/s, batch_nb=1874, gpu=0, loss=1.084, v_nb=1]\n",
            "Epoch 6:  59%|█████▉    | 2213/3750 [00:13<00:04, 352.23batch/s, batch_nb=1874, gpu=0, loss=1.084, v_nb=1]\n",
            "Epoch 6:  60%|██████    | 2253/3750 [00:13<00:04, 364.80batch/s, batch_nb=1874, gpu=0, loss=1.084, v_nb=1]\n",
            "Epoch 6:  61%|██████    | 2293/3750 [00:14<00:03, 372.60batch/s, batch_nb=1874, gpu=0, loss=1.084, v_nb=1]\n",
            "Epoch 6:  62%|██████▏   | 2333/3750 [00:14<00:03, 379.54batch/s, batch_nb=1874, gpu=0, loss=1.084, v_nb=1]\n",
            "Epoch 6:  63%|██████▎   | 2373/3750 [00:14<00:03, 383.77batch/s, batch_nb=1874, gpu=0, loss=1.084, v_nb=1]\n",
            "Epoch 6:  64%|██████▍   | 2414/3750 [00:14<00:03, 390.03batch/s, batch_nb=1874, gpu=0, loss=1.084, v_nb=1]\n",
            "Epoch 6:  65%|██████▌   | 2455/3750 [00:14<00:03, 394.12batch/s, batch_nb=1874, gpu=0, loss=1.084, v_nb=1]\n",
            "Epoch 6:  67%|██████▋   | 2495/3750 [00:14<00:03, 392.74batch/s, batch_nb=1874, gpu=0, loss=1.084, v_nb=1]\n",
            "Epoch 6:  68%|██████▊   | 2535/3750 [00:14<00:03, 387.68batch/s, batch_nb=1874, gpu=0, loss=1.084, v_nb=1]\n",
            "Epoch 6:  69%|██████▊   | 2574/3750 [00:14<00:03, 381.00batch/s, batch_nb=1874, gpu=0, loss=1.084, v_nb=1]\n",
            "Epoch 6:  70%|██████▉   | 2614/3750 [00:14<00:02, 386.21batch/s, batch_nb=1874, gpu=0, loss=1.084, v_nb=1]\n",
            "Epoch 6:  71%|███████   | 2653/3750 [00:14<00:02, 372.17batch/s, batch_nb=1874, gpu=0, loss=1.084, v_nb=1]\n",
            "Epoch 6:  72%|███████▏  | 2691/3750 [00:15<00:02, 373.62batch/s, batch_nb=1874, gpu=0, loss=1.084, v_nb=1]\n",
            "Epoch 6:  73%|███████▎  | 2730/3750 [00:15<00:02, 376.19batch/s, batch_nb=1874, gpu=0, loss=1.084, v_nb=1]\n",
            "Epoch 6:  74%|███████▍  | 2771/3750 [00:15<00:02, 383.61batch/s, batch_nb=1874, gpu=0, loss=1.084, v_nb=1]\n",
            "Epoch 6:  75%|███████▍  | 2811/3750 [00:15<00:02, 386.46batch/s, batch_nb=1874, gpu=0, loss=1.084, v_nb=1]\n",
            "Epoch 6:  76%|███████▌  | 2851/3750 [00:15<00:02, 389.43batch/s, batch_nb=1874, gpu=0, loss=1.084, v_nb=1]\n",
            "Epoch 6:  77%|███████▋  | 2891/3750 [00:15<00:02, 383.22batch/s, batch_nb=1874, gpu=0, loss=1.084, v_nb=1]\n",
            "Epoch 6:  78%|███████▊  | 2931/3750 [00:15<00:02, 385.35batch/s, batch_nb=1874, gpu=0, loss=1.084, v_nb=1]\n",
            "Epoch 6:  79%|███████▉  | 2970/3750 [00:15<00:02, 377.23batch/s, batch_nb=1874, gpu=0, loss=1.084, v_nb=1]\n",
            "Epoch 6:  80%|████████  | 3010/3750 [00:15<00:01, 382.92batch/s, batch_nb=1874, gpu=0, loss=1.084, v_nb=1]\n",
            "Epoch 6:  81%|████████▏ | 3051/3750 [00:16<00:01, 389.15batch/s, batch_nb=1874, gpu=0, loss=1.084, v_nb=1]\n",
            "Epoch 6:  82%|████████▏ | 3090/3750 [00:16<00:01, 388.66batch/s, batch_nb=1874, gpu=0, loss=1.084, v_nb=1]\n",
            "Epoch 6:  83%|████████▎ | 3130/3750 [00:16<00:01, 390.62batch/s, batch_nb=1874, gpu=0, loss=1.084, v_nb=1]\n",
            "Epoch 6:  85%|████████▍ | 3170/3750 [00:16<00:01, 390.42batch/s, batch_nb=1874, gpu=0, loss=1.084, v_nb=1]\n",
            "Epoch 6:  86%|████████▌ | 3210/3750 [00:16<00:01, 391.30batch/s, batch_nb=1874, gpu=0, loss=1.084, v_nb=1]\n",
            "Epoch 6:  87%|████████▋ | 3250/3750 [00:16<00:01, 390.98batch/s, batch_nb=1874, gpu=0, loss=1.084, v_nb=1]\n",
            "Epoch 6:  88%|████████▊ | 3290/3750 [00:16<00:01, 390.51batch/s, batch_nb=1874, gpu=0, loss=1.084, v_nb=1]\n",
            "Epoch 6:  89%|████████▉ | 3331/3750 [00:16<00:01, 393.61batch/s, batch_nb=1874, gpu=0, loss=1.084, v_nb=1]\n",
            "Epoch 6:  90%|████████▉ | 3371/3750 [00:16<00:00, 383.09batch/s, batch_nb=1874, gpu=0, loss=1.084, v_nb=1]\n",
            "Epoch 6:  91%|█████████ | 3410/3750 [00:16<00:00, 383.57batch/s, batch_nb=1874, gpu=0, loss=1.084, v_nb=1]\n",
            "Epoch 6:  92%|█████████▏| 3451/3750 [00:17<00:00, 388.98batch/s, batch_nb=1874, gpu=0, loss=1.084, v_nb=1]\n",
            "Epoch 6:  93%|█████████▎| 3490/3750 [00:17<00:00, 386.33batch/s, batch_nb=1874, gpu=0, loss=1.084, v_nb=1]\n",
            "Epoch 6:  94%|█████████▍| 3530/3750 [00:17<00:00, 388.77batch/s, batch_nb=1874, gpu=0, loss=1.084, v_nb=1]\n",
            "Epoch 6:  95%|█████████▌| 3569/3750 [00:17<00:00, 388.22batch/s, batch_nb=1874, gpu=0, loss=1.084, v_nb=1]\n",
            "Epoch 6:  96%|█████████▌| 3608/3750 [00:17<00:00, 386.57batch/s, batch_nb=1874, gpu=0, loss=1.084, v_nb=1]\n",
            "Epoch 6:  97%|█████████▋| 3647/3750 [00:17<00:00, 383.89batch/s, batch_nb=1874, gpu=0, loss=1.084, v_nb=1]\n",
            "Epoch 6:  98%|█████████▊| 3686/3750 [00:17<00:00, 374.31batch/s, batch_nb=1874, gpu=0, loss=1.084, v_nb=1]\n",
            "Epoch 6:  99%|█████████▉| 3725/3750 [00:17<00:00, 377.78batch/s, batch_nb=1874, gpu=0, loss=1.084, v_nb=1]\n",
            "Epoch 6: 100%|██████████| 3750/3750 [00:17<00:00, 377.78batch/s, batch_nb=1874, gpu=0, loss=1.084, v_nb=1]\n",
            "Epoch 7:  50%|█████     | 1875/3750 [00:12<00:12, 146.74batch/s, batch_nb=1874, gpu=0, loss=1.073, v_nb=1]\n",
            "Epoch 7:  51%|█████     | 1905/3750 [00:12<00:10, 175.60batch/s, batch_nb=1874, gpu=0, loss=1.073, v_nb=1]\n",
            "Epoch 7:  52%|█████▏    | 1941/3750 [00:12<00:08, 206.94batch/s, batch_nb=1874, gpu=0, loss=1.073, v_nb=1]\n",
            "Epoch 7:  53%|█████▎    | 1980/3750 [00:12<00:07, 239.87batch/s, batch_nb=1874, gpu=0, loss=1.073, v_nb=1]\n",
            "Epoch 7:  54%|█████▍    | 2021/3750 [00:13<00:06, 272.99batch/s, batch_nb=1874, gpu=0, loss=1.073, v_nb=1]\n",
            "Epoch 7:  55%|█████▍    | 2059/3750 [00:13<00:05, 297.45batch/s, batch_nb=1874, gpu=0, loss=1.073, v_nb=1]\n",
            "Epoch 7:  56%|█████▌    | 2099/3750 [00:13<00:05, 321.11batch/s, batch_nb=1874, gpu=0, loss=1.073, v_nb=1]\n",
            "Epoch 7:  57%|█████▋    | 2135/3750 [00:13<00:04, 330.83batch/s, batch_nb=1874, gpu=0, loss=1.073, v_nb=1]\n",
            "Epoch 7:  58%|█████▊    | 2174/3750 [00:13<00:04, 346.38batch/s, batch_nb=1874, gpu=0, loss=1.073, v_nb=1]\n",
            "Epoch 7:  59%|█████▉    | 2213/3750 [00:13<00:04, 355.40batch/s, batch_nb=1874, gpu=0, loss=1.073, v_nb=1]\n",
            "Epoch 7:  60%|██████    | 2252/3750 [00:13<00:04, 363.48batch/s, batch_nb=1874, gpu=0, loss=1.073, v_nb=1]\n",
            "Epoch 7:  61%|██████    | 2290/3750 [00:13<00:04, 364.72batch/s, batch_nb=1874, gpu=0, loss=1.073, v_nb=1]\n",
            "Epoch 7:  62%|██████▏   | 2328/3750 [00:13<00:03, 368.85batch/s, batch_nb=1874, gpu=0, loss=1.073, v_nb=1]\n",
            "Epoch 7:  63%|██████▎   | 2367/3750 [00:13<00:03, 373.22batch/s, batch_nb=1874, gpu=0, loss=1.073, v_nb=1]\n",
            "Epoch 7:  64%|██████▍   | 2407/3750 [00:14<00:03, 379.23batch/s, batch_nb=1874, gpu=0, loss=1.073, v_nb=1]\n",
            "Epoch 7:  65%|██████▌   | 2447/3750 [00:14<00:03, 384.49batch/s, batch_nb=1874, gpu=0, loss=1.073, v_nb=1]\n",
            "Epoch 7:  66%|██████▋   | 2486/3750 [00:14<00:03, 379.42batch/s, batch_nb=1874, gpu=0, loss=1.073, v_nb=1]\n",
            "Epoch 7:  67%|██████▋   | 2525/3750 [00:14<00:03, 364.67batch/s, batch_nb=1874, gpu=0, loss=1.073, v_nb=1]\n",
            "Epoch 7:  68%|██████▊   | 2562/3750 [00:14<00:03, 365.34batch/s, batch_nb=1874, gpu=0, loss=1.073, v_nb=1]\n",
            "Epoch 7:  69%|██████▉   | 2603/3750 [00:14<00:03, 375.54batch/s, batch_nb=1874, gpu=0, loss=1.073, v_nb=1]\n",
            "Epoch 7:  70%|███████   | 2641/3750 [00:14<00:02, 373.12batch/s, batch_nb=1874, gpu=0, loss=1.073, v_nb=1]\n",
            "Epoch 7:  71%|███████▏  | 2679/3750 [00:14<00:02, 373.38batch/s, batch_nb=1874, gpu=0, loss=1.073, v_nb=1]\n",
            "Epoch 7:  72%|███████▏  | 2717/3750 [00:14<00:02, 374.83batch/s, batch_nb=1874, gpu=0, loss=1.073, v_nb=1]\n",
            "Epoch 7:  73%|███████▎  | 2755/3750 [00:14<00:02, 367.35batch/s, batch_nb=1874, gpu=0, loss=1.073, v_nb=1]\n",
            "Epoch 7:  74%|███████▍  | 2793/3750 [00:15<00:02, 370.16batch/s, batch_nb=1874, gpu=0, loss=1.073, v_nb=1]\n",
            "Epoch 7:  76%|███████▌  | 2833/3750 [00:15<00:02, 377.69batch/s, batch_nb=1874, gpu=0, loss=1.073, v_nb=1]\n",
            "Epoch 7:  77%|███████▋  | 2873/3750 [00:15<00:02, 383.08batch/s, batch_nb=1874, gpu=0, loss=1.073, v_nb=1]\n",
            "Epoch 7:  78%|███████▊  | 2912/3750 [00:15<00:02, 375.74batch/s, batch_nb=1874, gpu=0, loss=1.073, v_nb=1]\n",
            "Epoch 7:  79%|███████▊  | 2950/3750 [00:15<00:02, 374.56batch/s, batch_nb=1874, gpu=0, loss=1.073, v_nb=1]\n",
            "Epoch 7:  80%|███████▉  | 2990/3750 [00:15<00:01, 380.47batch/s, batch_nb=1874, gpu=0, loss=1.073, v_nb=1]\n",
            "Epoch 7:  81%|████████  | 3029/3750 [00:15<00:01, 382.35batch/s, batch_nb=1874, gpu=0, loss=1.073, v_nb=1]\n",
            "Epoch 7:  82%|████████▏ | 3069/3750 [00:15<00:01, 385.25batch/s, batch_nb=1874, gpu=0, loss=1.073, v_nb=1]\n",
            "Epoch 7:  83%|████████▎ | 3108/3750 [00:15<00:01, 384.14batch/s, batch_nb=1874, gpu=0, loss=1.073, v_nb=1]\n",
            "Epoch 7:  84%|████████▍ | 3148/3750 [00:16<00:01, 386.66batch/s, batch_nb=1874, gpu=0, loss=1.073, v_nb=1]\n",
            "Epoch 7:  85%|████████▍ | 3187/3750 [00:16<00:01, 384.19batch/s, batch_nb=1874, gpu=0, loss=1.073, v_nb=1]\n",
            "Epoch 7:  86%|████████▌ | 3228/3750 [00:16<00:01, 389.30batch/s, batch_nb=1874, gpu=0, loss=1.073, v_nb=1]\n",
            "Epoch 7:  87%|████████▋ | 3267/3750 [00:16<00:01, 387.29batch/s, batch_nb=1874, gpu=0, loss=1.073, v_nb=1]\n",
            "Epoch 7:  88%|████████▊ | 3306/3750 [00:16<00:01, 381.99batch/s, batch_nb=1874, gpu=0, loss=1.073, v_nb=1]\n",
            "Epoch 7:  89%|████████▉ | 3345/3750 [00:16<00:01, 383.80batch/s, batch_nb=1874, gpu=0, loss=1.073, v_nb=1]\n",
            "Epoch 7:  90%|█████████ | 3385/3750 [00:16<00:00, 388.20batch/s, batch_nb=1874, gpu=0, loss=1.073, v_nb=1]\n",
            "Epoch 7:  91%|█████████▏| 3424/3750 [00:16<00:00, 384.36batch/s, batch_nb=1874, gpu=0, loss=1.073, v_nb=1]\n",
            "Epoch 7:  92%|█████████▏| 3464/3750 [00:16<00:00, 386.99batch/s, batch_nb=1874, gpu=0, loss=1.073, v_nb=1]\n",
            "Epoch 7:  93%|█████████▎| 3504/3750 [00:16<00:00, 388.55batch/s, batch_nb=1874, gpu=0, loss=1.073, v_nb=1]\n",
            "Epoch 7:  95%|█████████▍| 3544/3750 [00:17<00:00, 391.54batch/s, batch_nb=1874, gpu=0, loss=1.073, v_nb=1]\n",
            "Epoch 7:  96%|█████████▌| 3584/3750 [00:17<00:00, 390.96batch/s, batch_nb=1874, gpu=0, loss=1.073, v_nb=1]\n",
            "Epoch 7:  97%|█████████▋| 3624/3750 [00:17<00:00, 386.29batch/s, batch_nb=1874, gpu=0, loss=1.073, v_nb=1]\n",
            "Epoch 7:  98%|█████████▊| 3663/3750 [00:17<00:00, 381.79batch/s, batch_nb=1874, gpu=0, loss=1.073, v_nb=1]\n",
            "Epoch 7:  99%|█████████▊| 3702/3750 [00:17<00:00, 368.98batch/s, batch_nb=1874, gpu=0, loss=1.073, v_nb=1]\n",
            "Epoch 7: 100%|██████████| 3750/3750 [00:17<00:00, 374.17batch/s, batch_nb=1874, gpu=0, loss=1.073, v_nb=1]\n",
            "Epoch 8:  50%|█████     | 1875/3750 [00:13<00:12, 148.79batch/s, batch_nb=1874, gpu=0, loss=1.092, v_nb=1]\n",
            "Epoch 8:  51%|█████     | 1899/3750 [00:13<00:10, 172.97batch/s, batch_nb=1874, gpu=0, loss=1.092, v_nb=1]\n",
            "Epoch 8:  52%|█████▏    | 1939/3750 [00:13<00:08, 208.03batch/s, batch_nb=1874, gpu=0, loss=1.092, v_nb=1]\n",
            "Epoch 8:  53%|█████▎    | 1979/3750 [00:13<00:07, 242.91batch/s, batch_nb=1874, gpu=0, loss=1.092, v_nb=1]\n",
            "Epoch 8:  54%|█████▍    | 2020/3750 [00:13<00:06, 276.52batch/s, batch_nb=1874, gpu=0, loss=1.092, v_nb=1]\n",
            "Epoch 8:  55%|█████▍    | 2062/3750 [00:13<00:05, 306.94batch/s, batch_nb=1874, gpu=0, loss=1.092, v_nb=1]\n",
            "Epoch 8:  56%|█████▌    | 2102/3750 [00:13<00:04, 329.63batch/s, batch_nb=1874, gpu=0, loss=1.092, v_nb=1]\n",
            "Epoch 8:  57%|█████▋    | 2143/3750 [00:13<00:04, 348.75batch/s, batch_nb=1874, gpu=0, loss=1.092, v_nb=1]\n",
            "Epoch 8:  58%|█████▊    | 2183/3750 [00:13<00:04, 362.10batch/s, batch_nb=1874, gpu=0, loss=1.092, v_nb=1]\n",
            "Epoch 8:  59%|█████▉    | 2223/3750 [00:14<00:04, 372.01batch/s, batch_nb=1874, gpu=0, loss=1.092, v_nb=1]\n",
            "Epoch 8:  60%|██████    | 2264/3750 [00:14<00:03, 381.99batch/s, batch_nb=1874, gpu=0, loss=1.092, v_nb=1]\n",
            "Epoch 8:  61%|██████▏   | 2304/3750 [00:14<00:03, 378.60batch/s, batch_nb=1874, gpu=0, loss=1.092, v_nb=1]\n",
            "Epoch 8:  63%|██████▎   | 2344/3750 [00:14<00:03, 384.66batch/s, batch_nb=1874, gpu=0, loss=1.092, v_nb=1]\n",
            "Epoch 8:  64%|██████▎   | 2385/3750 [00:14<00:03, 390.50batch/s, batch_nb=1874, gpu=0, loss=1.092, v_nb=1]\n",
            "Epoch 8:  65%|██████▍   | 2426/3750 [00:14<00:03, 395.93batch/s, batch_nb=1874, gpu=0, loss=1.092, v_nb=1]\n",
            "Epoch 8:  66%|██████▌   | 2466/3750 [00:14<00:03, 391.08batch/s, batch_nb=1874, gpu=0, loss=1.092, v_nb=1]\n",
            "Epoch 8:  67%|██████▋   | 2506/3750 [00:14<00:03, 391.27batch/s, batch_nb=1874, gpu=0, loss=1.092, v_nb=1]\n",
            "Epoch 8:  68%|██████▊   | 2546/3750 [00:14<00:03, 367.02batch/s, batch_nb=1874, gpu=0, loss=1.092, v_nb=1]\n",
            "Epoch 8:  69%|██████▉   | 2586/3750 [00:15<00:03, 373.71batch/s, batch_nb=1874, gpu=0, loss=1.092, v_nb=1]\n",
            "Epoch 8:  70%|███████   | 2626/3750 [00:15<00:02, 379.05batch/s, batch_nb=1874, gpu=0, loss=1.092, v_nb=1]\n",
            "Epoch 8:  71%|███████   | 2665/3750 [00:15<00:02, 373.12batch/s, batch_nb=1874, gpu=0, loss=1.092, v_nb=1]\n",
            "Epoch 8:  72%|███████▏  | 2705/3750 [00:15<00:02, 380.47batch/s, batch_nb=1874, gpu=0, loss=1.092, v_nb=1]\n",
            "Epoch 8:  73%|███████▎  | 2747/3750 [00:15<00:02, 389.77batch/s, batch_nb=1874, gpu=0, loss=1.092, v_nb=1]\n",
            "Epoch 8:  74%|███████▍  | 2787/3750 [00:15<00:02, 390.57batch/s, batch_nb=1874, gpu=0, loss=1.092, v_nb=1]\n",
            "Epoch 8:  75%|███████▌  | 2827/3750 [00:15<00:02, 393.19batch/s, batch_nb=1874, gpu=0, loss=1.092, v_nb=1]\n",
            "Epoch 8:  76%|███████▋  | 2867/3750 [00:15<00:02, 393.56batch/s, batch_nb=1874, gpu=0, loss=1.092, v_nb=1]\n",
            "Epoch 8:  78%|███████▊  | 2908/3750 [00:15<00:02, 395.97batch/s, batch_nb=1874, gpu=0, loss=1.092, v_nb=1]\n",
            "Epoch 8:  79%|███████▊  | 2949/3750 [00:15<00:02, 397.64batch/s, batch_nb=1874, gpu=0, loss=1.092, v_nb=1]\n",
            "Epoch 8:  80%|███████▉  | 2990/3750 [00:16<00:01, 399.12batch/s, batch_nb=1874, gpu=0, loss=1.092, v_nb=1]\n",
            "Epoch 8:  81%|████████  | 3030/3750 [00:16<00:01, 393.67batch/s, batch_nb=1874, gpu=0, loss=1.092, v_nb=1]\n",
            "Epoch 8:  82%|████████▏ | 3070/3750 [00:16<00:01, 384.73batch/s, batch_nb=1874, gpu=0, loss=1.092, v_nb=1]\n",
            "Epoch 8:  83%|████████▎ | 3111/3750 [00:16<00:01, 391.32batch/s, batch_nb=1874, gpu=0, loss=1.092, v_nb=1]\n",
            "Epoch 8:  84%|████████▍ | 3151/3750 [00:16<00:01, 378.89batch/s, batch_nb=1874, gpu=0, loss=1.092, v_nb=1]\n",
            "Epoch 8:  85%|████████▌ | 3193/3750 [00:16<00:01, 388.07batch/s, batch_nb=1874, gpu=0, loss=1.092, v_nb=1]\n",
            "Epoch 8:  86%|████████▌ | 3232/3750 [00:16<00:01, 388.63batch/s, batch_nb=1874, gpu=0, loss=1.092, v_nb=1]\n",
            "Epoch 8:  87%|████████▋ | 3273/3750 [00:16<00:01, 393.81batch/s, batch_nb=1874, gpu=0, loss=1.092, v_nb=1]\n",
            "Epoch 8:  88%|████████▊ | 3315/3750 [00:16<00:01, 399.59batch/s, batch_nb=1874, gpu=0, loss=1.092, v_nb=1]\n",
            "Epoch 8:  89%|████████▉ | 3356/3750 [00:16<00:00, 399.87batch/s, batch_nb=1874, gpu=0, loss=1.092, v_nb=1]\n",
            "Epoch 8:  91%|█████████ | 3397/3750 [00:17<00:00, 401.90batch/s, batch_nb=1874, gpu=0, loss=1.092, v_nb=1]\n",
            "Epoch 8:  92%|█████████▏| 3438/3750 [00:17<00:00, 389.37batch/s, batch_nb=1874, gpu=0, loss=1.092, v_nb=1]\n",
            "Epoch 8:  93%|█████████▎| 3478/3750 [00:17<00:00, 376.44batch/s, batch_nb=1874, gpu=0, loss=1.092, v_nb=1]\n",
            "Epoch 8:  94%|█████████▍| 3519/3750 [00:17<00:00, 384.07batch/s, batch_nb=1874, gpu=0, loss=1.092, v_nb=1]\n",
            "Epoch 8:  95%|█████████▍| 3559/3750 [00:17<00:00, 386.58batch/s, batch_nb=1874, gpu=0, loss=1.092, v_nb=1]\n",
            "Epoch 8:  96%|█████████▌| 3601/3750 [00:17<00:00, 393.90batch/s, batch_nb=1874, gpu=0, loss=1.092, v_nb=1]\n",
            "Epoch 8:  97%|█████████▋| 3641/3750 [00:17<00:00, 395.65batch/s, batch_nb=1874, gpu=0, loss=1.092, v_nb=1]\n",
            "Epoch 8:  98%|█████████▊| 3681/3750 [00:17<00:00, 396.30batch/s, batch_nb=1874, gpu=0, loss=1.092, v_nb=1]\n",
            "Epoch 8:  99%|█████████▉| 3722/3750 [00:17<00:00, 398.98batch/s, batch_nb=1874, gpu=0, loss=1.092, v_nb=1]\n",
            "Epoch 8: 100%|██████████| 3750/3750 [00:17<00:00, 398.98batch/s, batch_nb=1874, gpu=0, loss=1.092, v_nb=1]\n",
            "Epoch 9:  50%|█████     | 1875/3750 [00:13<00:13, 134.91batch/s, batch_nb=1874, gpu=0, loss=1.076, v_nb=1]\n",
            "Epoch 9:  51%|█████     | 1914/3750 [00:13<00:10, 167.43batch/s, batch_nb=1874, gpu=0, loss=1.076, v_nb=1]\n",
            "Epoch 9:  52%|█████▏    | 1954/3750 [00:13<00:08, 202.60batch/s, batch_nb=1874, gpu=0, loss=1.076, v_nb=1]\n",
            "Epoch 9:  53%|█████▎    | 1994/3750 [00:13<00:07, 237.28batch/s, batch_nb=1874, gpu=0, loss=1.076, v_nb=1]\n",
            "Epoch 9:  54%|█████▍    | 2032/3750 [00:13<00:06, 267.29batch/s, batch_nb=1874, gpu=0, loss=1.076, v_nb=1]\n",
            "Epoch 9:  55%|█████▌    | 2067/3750 [00:13<00:05, 286.61batch/s, batch_nb=1874, gpu=0, loss=1.076, v_nb=1]\n",
            "Epoch 9:  56%|█████▌    | 2105/3750 [00:13<00:05, 308.01batch/s, batch_nb=1874, gpu=0, loss=1.076, v_nb=1]\n",
            "Epoch 9:  57%|█████▋    | 2144/3750 [00:13<00:04, 328.15batch/s, batch_nb=1874, gpu=0, loss=1.076, v_nb=1]\n",
            "Epoch 9:  58%|█████▊    | 2180/3750 [00:13<00:04, 336.42batch/s, batch_nb=1874, gpu=0, loss=1.076, v_nb=1]\n",
            "Epoch 9:  59%|█████▉    | 2218/3750 [00:14<00:04, 347.20batch/s, batch_nb=1874, gpu=0, loss=1.076, v_nb=1]\n",
            "Epoch 9:  60%|██████    | 2256/3750 [00:14<00:04, 355.95batch/s, batch_nb=1874, gpu=0, loss=1.076, v_nb=1]\n",
            "Epoch 9:  61%|██████    | 2294/3750 [00:14<00:04, 361.36batch/s, batch_nb=1874, gpu=0, loss=1.076, v_nb=1]\n",
            "Epoch 9:  62%|██████▏   | 2333/3750 [00:14<00:03, 369.04batch/s, batch_nb=1874, gpu=0, loss=1.076, v_nb=1]\n",
            "Epoch 9:  63%|██████▎   | 2371/3750 [00:14<00:03, 371.58batch/s, batch_nb=1874, gpu=0, loss=1.076, v_nb=1]\n",
            "Epoch 9:  64%|██████▍   | 2412/3750 [00:14<00:03, 380.12batch/s, batch_nb=1874, gpu=0, loss=1.076, v_nb=1]\n",
            "Epoch 9:  65%|██████▌   | 2451/3750 [00:14<00:03, 377.20batch/s, batch_nb=1874, gpu=0, loss=1.076, v_nb=1]\n",
            "Epoch 9:  66%|██████▋   | 2491/3750 [00:14<00:03, 383.67batch/s, batch_nb=1874, gpu=0, loss=1.076, v_nb=1]\n",
            "Epoch 9:  67%|██████▋   | 2530/3750 [00:14<00:03, 382.75batch/s, batch_nb=1874, gpu=0, loss=1.076, v_nb=1]\n",
            "Epoch 9:  69%|██████▊   | 2571/3750 [00:14<00:03, 389.01batch/s, batch_nb=1874, gpu=0, loss=1.076, v_nb=1]\n",
            "Epoch 9:  70%|██████▉   | 2611/3750 [00:15<00:02, 386.02batch/s, batch_nb=1874, gpu=0, loss=1.076, v_nb=1]\n",
            "Epoch 9:  71%|███████   | 2651/3750 [00:15<00:02, 389.65batch/s, batch_nb=1874, gpu=0, loss=1.076, v_nb=1]\n",
            "Epoch 9:  72%|███████▏  | 2692/3750 [00:15<00:02, 393.30batch/s, batch_nb=1874, gpu=0, loss=1.076, v_nb=1]\n",
            "Epoch 9:  73%|███████▎  | 2733/3750 [00:15<00:02, 396.08batch/s, batch_nb=1874, gpu=0, loss=1.076, v_nb=1]\n",
            "Epoch 9:  74%|███████▍  | 2774/3750 [00:15<00:02, 397.73batch/s, batch_nb=1874, gpu=0, loss=1.076, v_nb=1]\n",
            "Epoch 9:  75%|███████▌  | 2815/3750 [00:15<00:02, 399.76batch/s, batch_nb=1874, gpu=0, loss=1.076, v_nb=1]\n",
            "Validating:  50%|█████     | 941/1875 [00:02<00:02, 399.76batch/s]\u001b[A\n",
            "Epoch 9:  76%|███████▌  | 2856/3750 [00:15<00:02, 390.43batch/s, batch_nb=1874, gpu=0, loss=1.076, v_nb=1]\n",
            "Epoch 9:  77%|███████▋  | 2896/3750 [00:15<00:02, 388.36batch/s, batch_nb=1874, gpu=0, loss=1.076, v_nb=1]\n",
            "Epoch 9:  79%|███████▉  | 2975/3750 [00:16<00:02, 372.89batch/s, batch_nb=1874, gpu=0, loss=1.076, v_nb=1]\n",
            "Epoch 9:  80%|████████  | 3013/3750 [00:16<00:01, 370.25batch/s, batch_nb=1874, gpu=0, loss=1.076, v_nb=1]\n",
            "Validating:  61%|██████    | 1139/1875 [00:02<00:02, 367.57batch/s]\u001b[A\n",
            "Epoch 9:  82%|████████▏ | 3087/3750 [00:16<00:01, 355.88batch/s, batch_nb=1874, gpu=0, loss=1.076, v_nb=1]\n",
            "Epoch 9:  83%|████████▎ | 3125/3750 [00:16<00:01, 361.60batch/s, batch_nb=1874, gpu=0, loss=1.076, v_nb=1]\n",
            "Epoch 9:  84%|████████▍ | 3163/3750 [00:16<00:01, 365.66batch/s, batch_nb=1874, gpu=0, loss=1.076, v_nb=1]\n",
            "Epoch 9:  85%|████████▌ | 3202/3750 [00:16<00:01, 371.29batch/s, batch_nb=1874, gpu=0, loss=1.076, v_nb=1]\n",
            "Epoch 9:  86%|████████▋ | 3240/3750 [00:16<00:01, 372.07batch/s, batch_nb=1874, gpu=0, loss=1.076, v_nb=1]\n",
            "Epoch 9:  87%|████████▋ | 3279/3750 [00:16<00:01, 376.29batch/s, batch_nb=1874, gpu=0, loss=1.076, v_nb=1]\n",
            "Epoch 9:  88%|████████▊ | 3318/3750 [00:16<00:01, 379.89batch/s, batch_nb=1874, gpu=0, loss=1.076, v_nb=1]\n",
            "Epoch 9:  90%|████████▉ | 3358/3750 [00:17<00:01, 384.67batch/s, batch_nb=1874, gpu=0, loss=1.076, v_nb=1]\n",
            "Epoch 9:  91%|█████████ | 3397/3750 [00:17<00:00, 380.46batch/s, batch_nb=1874, gpu=0, loss=1.076, v_nb=1]\n",
            "Epoch 9:  92%|█████████▏| 3436/3750 [00:17<00:00, 378.31batch/s, batch_nb=1874, gpu=0, loss=1.076, v_nb=1]\n",
            "Epoch 9:  93%|█████████▎| 3475/3750 [00:17<00:00, 381.31batch/s, batch_nb=1874, gpu=0, loss=1.076, v_nb=1]\n",
            "Epoch 9:  94%|█████████▎| 3514/3750 [00:17<00:00, 382.46batch/s, batch_nb=1874, gpu=0, loss=1.076, v_nb=1]\n",
            "Epoch 9:  95%|█████████▍| 3553/3750 [00:17<00:00, 378.88batch/s, batch_nb=1874, gpu=0, loss=1.076, v_nb=1]\n",
            "Epoch 9:  96%|█████████▌| 3591/3750 [00:17<00:00, 374.09batch/s, batch_nb=1874, gpu=0, loss=1.076, v_nb=1]\n",
            "Epoch 9:  97%|█████████▋| 3629/3750 [00:17<00:00, 364.76batch/s, batch_nb=1874, gpu=0, loss=1.076, v_nb=1]\n",
            "Epoch 9:  98%|█████████▊| 3666/3750 [00:17<00:00, 365.07batch/s, batch_nb=1874, gpu=0, loss=1.076, v_nb=1]\n",
            "Epoch 9:  99%|█████████▉| 3704/3750 [00:17<00:00, 368.61batch/s, batch_nb=1874, gpu=0, loss=1.076, v_nb=1]\n",
            "Epoch 9: 100%|█████████▉| 3741/3750 [00:18<00:00, 368.16batch/s, batch_nb=1874, gpu=0, loss=1.076, v_nb=1]\n",
            "Epoch 9: 100%|██████████| 3750/3750 [00:18<00:00, 368.16batch/s, batch_nb=1874, gpu=0, loss=1.076, v_nb=1]\n",
            "Epoch 10:  50%|█████     | 1875/3750 [00:13<00:13, 141.93batch/s, batch_nb=1874, gpu=0, loss=1.072, v_nb=1]\n",
            "Epoch 10:  50%|█████     | 1877/3750 [00:13<00:13, 141.94batch/s, batch_nb=1874, gpu=0, loss=1.072, v_nb=1]\n",
            "Epoch 10:  51%|█████     | 1913/3750 [00:13<00:10, 173.45batch/s, batch_nb=1874, gpu=0, loss=1.072, v_nb=1]\n",
            "Epoch 10:  52%|█████▏    | 1952/3750 [00:13<00:08, 207.98batch/s, batch_nb=1874, gpu=0, loss=1.072, v_nb=1]\n",
            "Epoch 10:  53%|█████▎    | 1992/3750 [00:13<00:07, 241.73batch/s, batch_nb=1874, gpu=0, loss=1.072, v_nb=1]\n",
            "Epoch 10:  54%|█████▍    | 2032/3750 [00:13<00:06, 273.54batch/s, batch_nb=1874, gpu=0, loss=1.072, v_nb=1]\n",
            "Epoch 10:  55%|█████▌    | 2070/3750 [00:13<00:05, 297.23batch/s, batch_nb=1874, gpu=0, loss=1.072, v_nb=1]\n",
            "Epoch 10:  56%|█████▌    | 2106/3750 [00:13<00:05, 313.42batch/s, batch_nb=1874, gpu=0, loss=1.072, v_nb=1]\n",
            "Epoch 10:  57%|█████▋    | 2147/3750 [00:14<00:04, 335.59batch/s, batch_nb=1874, gpu=0, loss=1.072, v_nb=1]\n",
            "Epoch 10:  58%|█████▊    | 2187/3750 [00:14<00:04, 352.12batch/s, batch_nb=1874, gpu=0, loss=1.072, v_nb=1]\n",
            "Epoch 10:  59%|█████▉    | 2228/3750 [00:14<00:04, 364.87batch/s, batch_nb=1874, gpu=0, loss=1.072, v_nb=1]\n",
            "Epoch 10:  60%|██████    | 2267/3750 [00:14<00:04, 369.82batch/s, batch_nb=1874, gpu=0, loss=1.072, v_nb=1]\n",
            "Epoch 10:  62%|██████▏   | 2308/3750 [00:14<00:03, 380.88batch/s, batch_nb=1874, gpu=0, loss=1.072, v_nb=1]\n",
            "Epoch 10:  63%|██████▎   | 2350/3750 [00:14<00:03, 389.30batch/s, batch_nb=1874, gpu=0, loss=1.072, v_nb=1]\n",
            "Epoch 10:  64%|██████▍   | 2391/3750 [00:14<00:03, 394.60batch/s, batch_nb=1874, gpu=0, loss=1.072, v_nb=1]\n",
            "Epoch 10:  65%|██████▍   | 2431/3750 [00:14<00:03, 395.75batch/s, batch_nb=1874, gpu=0, loss=1.072, v_nb=1]\n",
            "Epoch 10:  66%|██████▌   | 2471/3750 [00:14<00:03, 391.66batch/s, batch_nb=1874, gpu=0, loss=1.072, v_nb=1]\n",
            "Epoch 10:  67%|██████▋   | 2511/3750 [00:14<00:03, 383.90batch/s, batch_nb=1874, gpu=0, loss=1.072, v_nb=1]\n",
            "Epoch 10:  68%|██████▊   | 2552/3750 [00:15<00:03, 390.96batch/s, batch_nb=1874, gpu=0, loss=1.072, v_nb=1]\n",
            "Epoch 10:  69%|██████▉   | 2594/3750 [00:15<00:02, 397.06batch/s, batch_nb=1874, gpu=0, loss=1.072, v_nb=1]\n",
            "Epoch 10:  70%|███████   | 2634/3750 [00:15<00:02, 391.91batch/s, batch_nb=1874, gpu=0, loss=1.072, v_nb=1]\n",
            "Epoch 10:  71%|███████▏  | 2674/3750 [00:15<00:02, 389.27batch/s, batch_nb=1874, gpu=0, loss=1.072, v_nb=1]\n",
            "Epoch 10:  72%|███████▏  | 2715/3750 [00:15<00:02, 392.32batch/s, batch_nb=1874, gpu=0, loss=1.072, v_nb=1]\n",
            "Epoch 10:  73%|███████▎  | 2756/3750 [00:15<00:02, 394.28batch/s, batch_nb=1874, gpu=0, loss=1.072, v_nb=1]\n",
            "Epoch 10:  75%|███████▍  | 2797/3750 [00:15<00:02, 395.61batch/s, batch_nb=1874, gpu=0, loss=1.072, v_nb=1]\n",
            "Epoch 10:  76%|███████▌  | 2838/3750 [00:15<00:02, 399.23batch/s, batch_nb=1874, gpu=0, loss=1.072, v_nb=1]\n",
            "Epoch 10:  77%|███████▋  | 2880/3750 [00:15<00:02, 403.16batch/s, batch_nb=1874, gpu=0, loss=1.072, v_nb=1]\n",
            "Epoch 10:  78%|███████▊  | 2921/3750 [00:16<00:02, 385.64batch/s, batch_nb=1874, gpu=0, loss=1.072, v_nb=1]\n",
            "Epoch 10:  79%|███████▉  | 2960/3750 [00:16<00:02, 386.52batch/s, batch_nb=1874, gpu=0, loss=1.072, v_nb=1]\n",
            "Epoch 10:  80%|████████  | 3001/3750 [00:16<00:01, 391.77batch/s, batch_nb=1874, gpu=0, loss=1.072, v_nb=1]\n",
            "Epoch 10:  81%|████████  | 3043/3750 [00:16<00:01, 395.45batch/s, batch_nb=1874, gpu=0, loss=1.072, v_nb=1]\n",
            "Epoch 10:  82%|████████▏ | 3083/3750 [00:16<00:01, 396.52batch/s, batch_nb=1874, gpu=0, loss=1.072, v_nb=1]\n",
            "Epoch 10:  83%|████████▎ | 3123/3750 [00:16<00:01, 395.31batch/s, batch_nb=1874, gpu=0, loss=1.072, v_nb=1]\n",
            "Epoch 10:  84%|████████▍ | 3164/3750 [00:16<00:01, 399.22batch/s, batch_nb=1874, gpu=0, loss=1.072, v_nb=1]\n",
            "Epoch 10:  85%|████████▌ | 3205/3750 [00:16<00:01, 400.72batch/s, batch_nb=1874, gpu=0, loss=1.072, v_nb=1]\n",
            "Epoch 10:  87%|████████▋ | 3246/3750 [00:16<00:01, 402.02batch/s, batch_nb=1874, gpu=0, loss=1.072, v_nb=1]\n",
            "Epoch 10:  88%|████████▊ | 3287/3750 [00:16<00:01, 401.47batch/s, batch_nb=1874, gpu=0, loss=1.072, v_nb=1]\n",
            "Epoch 10:  89%|████████▊ | 3328/3750 [00:17<00:01, 386.27batch/s, batch_nb=1874, gpu=0, loss=1.072, v_nb=1]\n",
            "Epoch 10:  90%|████████▉ | 3369/3750 [00:17<00:00, 390.69batch/s, batch_nb=1874, gpu=0, loss=1.072, v_nb=1]\n",
            "Epoch 10:  91%|█████████ | 3410/3750 [00:17<00:00, 395.60batch/s, batch_nb=1874, gpu=0, loss=1.072, v_nb=1]\n",
            "Epoch 10:  92%|█████████▏| 3450/3750 [00:17<00:00, 395.56batch/s, batch_nb=1874, gpu=0, loss=1.072, v_nb=1]\n",
            "Epoch 10:  93%|█████████▎| 3491/3750 [00:17<00:00, 397.19batch/s, batch_nb=1874, gpu=0, loss=1.072, v_nb=1]\n",
            "Epoch 10:  94%|█████████▍| 3532/3750 [00:17<00:00, 399.46batch/s, batch_nb=1874, gpu=0, loss=1.072, v_nb=1]\n",
            "Epoch 10:  95%|█████████▌| 3572/3750 [00:17<00:00, 394.63batch/s, batch_nb=1874, gpu=0, loss=1.072, v_nb=1]\n",
            "Epoch 10:  96%|█████████▋| 3612/3750 [00:17<00:00, 385.70batch/s, batch_nb=1874, gpu=0, loss=1.072, v_nb=1]\n",
            "Epoch 10:  97%|█████████▋| 3651/3750 [00:17<00:00, 386.41batch/s, batch_nb=1874, gpu=0, loss=1.072, v_nb=1]\n",
            "Epoch 10:  98%|█████████▊| 3692/3750 [00:17<00:00, 392.10batch/s, batch_nb=1874, gpu=0, loss=1.072, v_nb=1]\n",
            "Epoch 10: 100%|██████████| 3750/3750 [00:18<00:00, 378.54batch/s, batch_nb=1874, gpu=0, loss=1.072, v_nb=1]\n",
            "                                                                   \u001b[AINFO:root:Epoch 00010: early stopping\n",
            "Epoch 10: 100%|██████████| 3750/3750 [00:18<00:00, 206.47batch/s, batch_nb=1874, gpu=0, loss=1.072, v_nb=1]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xF9-ouAEGFlZ",
        "colab_type": "text"
      },
      "source": [
        "By using the trainer you automatically get:\n",
        "1. Tensorboard logging\n",
        "2. Model checkpointing\n",
        "3. Training and validation loop\n",
        "4. early-stopping"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "18STRwHg-kW8",
        "colab_type": "text"
      },
      "source": [
        "#### Bonus\n",
        "In fact, if you keep calling fit, it'll keep training the model where it left off!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U2d1gc4N5IJX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trainer.fit(mnist_model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y836LVrOBgJk",
        "colab_type": "text"
      },
      "source": [
        "#### Test set\n",
        "Once you've trained using the train, val sets AND are done searching for hyperparameters, updating your model we want to run the test set!\n",
        "\n",
        "There are 2 ways of doing that:\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d23XjFPpB0zb",
        "colab_type": "text"
      },
      "source": [
        "#### Test with current trainer\n",
        "Use the same trainer you just used to test your model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EgVyzcKiB8fr",
        "colab_type": "code",
        "outputId": "d63c945f-8aa7-4349-cd65-5bb3736b0820",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "trainer.test()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Testing: 100%|██████████| 313/313 [00:01<00:00, 170.17batch/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iUGNgeG6Em__",
        "colab_type": "code",
        "outputId": "b4ce9dd2-55ed-4d15-f884-43518f29afe8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "trainer.tqdm_metrics"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'test_loss': 1.3869715929031372}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P0bSmCw57aV5",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "## GAN Example\n",
        "\n",
        "How to train a GAN!\n",
        "\n",
        "Main takeaways:\n",
        "1. Generator and discriminator are arbitraty PyTorch modules.\n",
        "2. training_step does both the generator and discriminator training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pBhBR3QJ7mhx",
        "colab_type": "text"
      },
      "source": [
        "## A. Generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mesU_huG-rr6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "To run this template just do:\n",
        "python gan.py\n",
        "After a few epochs, launch tensorboard to see the images being generated at every batch.\n",
        "tensorboard --logdir default\n",
        "\"\"\"\n",
        "import os\n",
        "from argparse import ArgumentParser\n",
        "from collections import OrderedDict\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import MNIST\n",
        "\n",
        "import pytorch_lightning as pl\n",
        "\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, latent_dim, img_shape):\n",
        "        super(Generator, self).__init__()\n",
        "        self.img_shape = img_shape\n",
        "\n",
        "        def block(in_feat, out_feat, normalize=True):\n",
        "            layers = [nn.Linear(in_feat, out_feat)]\n",
        "            if normalize:\n",
        "                layers.append(nn.BatchNorm1d(out_feat, 0.8))\n",
        "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
        "            return layers\n",
        "\n",
        "        self.model = nn.Sequential(\n",
        "            *block(latent_dim, 128, normalize=False),\n",
        "            *block(128, 256),\n",
        "            *block(256, 512),\n",
        "            *block(512, 1024),\n",
        "            nn.Linear(1024, int(np.prod(img_shape))),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, z):\n",
        "        img = self.model(z)\n",
        "        img = img.view(img.size(0), *self.img_shape)\n",
        "        return img"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bt37ycLx7uO3",
        "colab_type": "text"
      },
      "source": [
        "## B. Discriminator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pcPCt8JG7tI-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, img_shape):\n",
        "        super(Discriminator, self).__init__()\n",
        "\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(int(np.prod(img_shape)), 512),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Linear(256, 1),\n",
        "            nn.Sigmoid(),\n",
        "        )\n",
        "\n",
        "    def forward(self, img):\n",
        "        img_flat = img.view(img.size(0), -1)\n",
        "        validity = self.model(img_flat)\n",
        "\n",
        "        return validity"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TyYOdg8g77P0",
        "colab_type": "text"
      },
      "source": [
        "## C. GAN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ArrPXFM371jR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class GAN(pl.LightningModule):\n",
        "\n",
        "    def __init__(self, hparams):\n",
        "        super(GAN, self).__init__()\n",
        "        self.hparams = hparams\n",
        "\n",
        "        # networks\n",
        "        mnist_shape = (1, 28, 28)\n",
        "        self.generator = Generator(latent_dim=hparams.latent_dim, img_shape=mnist_shape)\n",
        "        self.discriminator = Discriminator(img_shape=mnist_shape)\n",
        "\n",
        "        # cache for generated images\n",
        "        self.generated_imgs = None\n",
        "        self.last_imgs = None\n",
        "\n",
        "    def forward(self, z):\n",
        "        return self.generator(z)\n",
        "\n",
        "    def adversarial_loss(self, y_hat, y):\n",
        "        return F.binary_cross_entropy(y_hat, y)\n",
        "\n",
        "    def training_step(self, batch, batch_nb, optimizer_i):\n",
        "        imgs, _ = batch\n",
        "        self.last_imgs = imgs\n",
        "\n",
        "        # train generator\n",
        "        if optimizer_i == 0:\n",
        "            # sample noise\n",
        "            z = torch.randn(imgs.shape[0], self.hparams.latent_dim)\n",
        "\n",
        "            # match gpu device (or keep as cpu)\n",
        "            if self.on_gpu:\n",
        "                z = z.cuda(imgs.device.index)\n",
        "\n",
        "            # generate images\n",
        "            self.generated_imgs = self.forward(z)\n",
        "\n",
        "            # log sampled images\n",
        "            # sample_imgs = self.generated_imgs[:6]\n",
        "            # grid = torchvision.utils.make_grid(sample_imgs)\n",
        "            # self.logger.experiment.add_image('generated_images', grid, 0)\n",
        "\n",
        "            # ground truth result (ie: all fake)\n",
        "            # put on GPU because we created this tensor inside training_loop\n",
        "            valid = torch.ones(imgs.size(0), 1)\n",
        "            if self.on_gpu:\n",
        "              valid = valid.cuda(imgs.device.index)\n",
        "\n",
        "            # adversarial loss is binary cross-entropy\n",
        "            g_loss = self.adversarial_loss(self.discriminator(self.generated_imgs), valid)\n",
        "            tqdm_dict = {'g_loss': g_loss}\n",
        "            output = OrderedDict({\n",
        "                'loss': g_loss,\n",
        "                'progress_bar': tqdm_dict,\n",
        "                'log': tqdm_dict\n",
        "            })\n",
        "            return output\n",
        "\n",
        "        # train discriminator\n",
        "        if optimizer_i == 1:\n",
        "            # Measure discriminator's ability to classify real from generated samples\n",
        "\n",
        "            # how well can it label as real?\n",
        "            valid = torch.ones(imgs.size(0), 1)\n",
        "            if self.on_gpu:\n",
        "              valid = valid.cuda(imgs.device.index)\n",
        "\n",
        "            real_loss = self.adversarial_loss(self.discriminator(imgs), valid)\n",
        "\n",
        "            # how well can it label as fake?\n",
        "            fake = torch.zeros(imgs.size(0), 1)\n",
        "            if self.on_gpu:\n",
        "              fake = fake.cuda(imgs.device.index)\n",
        "\n",
        "            fake_loss = self.adversarial_loss(\n",
        "                self.discriminator(self.generated_imgs.detach()), fake)\n",
        "\n",
        "            # discriminator loss is the average of these\n",
        "            d_loss = (real_loss + fake_loss) / 2\n",
        "            tqdm_dict = {'d_loss': d_loss}\n",
        "            output = OrderedDict({\n",
        "                'loss': d_loss,\n",
        "                'progress_bar': tqdm_dict,\n",
        "                'log': tqdm_dict\n",
        "            })\n",
        "            return output\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        lr = self.hparams.lr\n",
        "        b1 = self.hparams.b1\n",
        "        b2 = self.hparams.b2\n",
        "\n",
        "        opt_g = torch.optim.Adam(self.generator.parameters(), lr=lr, betas=(b1, b2))\n",
        "        opt_d = torch.optim.Adam(self.discriminator.parameters(), lr=lr, betas=(b1, b2))\n",
        "        return [opt_g, opt_d], []\n",
        "\n",
        "    @pl.data_loader\n",
        "    def train_dataloader(self):\n",
        "        transform = transforms.Compose([transforms.ToTensor(),\n",
        "                                        transforms.Normalize([0.5], [0.5])])\n",
        "        dataset = MNIST(os.getcwd(), train=True, download=True, transform=transform)\n",
        "        return DataLoader(dataset, batch_size=self.hparams.batch_size)\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        z = torch.randn(8, self.hparams.latent_dim)\n",
        "        # match gpu device (or keep as cpu)\n",
        "        if self.on_gpu:\n",
        "            z = z.cuda(self.last_imgs.device.index)\n",
        "\n",
        "        # log sampled images\n",
        "        sample_imgs = self.forward(z)\n",
        "        grid = torchvision.utils.make_grid(sample_imgs)\n",
        "        self.logger.experiment.add_image(f'generated_images', grid, self.current_epoch)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-WRY6dfn8ScZ",
        "colab_type": "text"
      },
      "source": [
        "### D. Trainer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xsmHHcpP8ryX",
        "colab_type": "text"
      },
      "source": [
        "Here we fake using argparse"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fIJl3phH8uEI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from argparse import Namespace\n",
        "\n",
        "args = {\n",
        "    'batch_size': 32,\n",
        "    'lr': 0.0002,\n",
        "    'b1': 0.5,\n",
        "    'b2': 0.999,\n",
        "    'latent_dim': 100\n",
        "}\n",
        "hparams = Namespace(**args)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h788dCGu7_Iu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gan_model = GAN(hparams)\n",
        "\n",
        "# most basic trainer, uses good defaults (1 gpu)\n",
        "trainer = pl.Trainer(gpus=1)    \n",
        "trainer.fit(gan_model)   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7uQVI-xv9Ddj",
        "colab_type": "text"
      },
      "source": [
        "---  \n",
        "## BERT example\n",
        "BERT + Lightning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e2npX-Gi9uwa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! pip install transformers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DeLyZQ_E9o1T",
        "colab_type": "text"
      },
      "source": [
        "#### Data download + processing\n",
        "\n",
        "Let's grab the correct data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eBP6FeY18_Ck",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers.data.processors.glue import MnliProcessor\n",
        "import torch\n",
        "from transformers import (\n",
        "    BertModel,\n",
        "    BertTokenizer\n",
        ")\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
        "bert = BertModel.from_pretrained('bert-base-cased', output_attentions=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vMbozzxs9xq_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import sys\n",
        "import shutil\n",
        "import argparse\n",
        "import tempfile\n",
        "import urllib.request\n",
        "import zipfile\n",
        "\n",
        "TASKS = [\"CoLA\", \"SST\", \"MRPC\", \"QQP\", \"STS\", \"MNLI\", \"SNLI\", \"QNLI\", \"RTE\", \"WNLI\", \"diagnostic\"]\n",
        "TASK2PATH = {\n",
        "    \"CoLA\": \"https://firebasestorage.googleapis.com/v0/b/mtl-sentence-representations.appspot.com/o/data%2FCoLA.zip?alt=media&token=46d5e637-3411-4188-bc44-5809b5bfb5f4\",  # noqa\n",
        "    \"SST\": \"https://firebasestorage.googleapis.com/v0/b/mtl-sentence-representations.appspot.com/o/data%2FSST-2.zip?alt=media&token=aabc5f6b-e466-44a2-b9b4-cf6337f84ac8\",  # noqa\n",
        "    \"MRPC\": \"https://firebasestorage.googleapis.com/v0/b/mtl-sentence-representations.appspot.com/o/data%2Fmrpc_dev_ids.tsv?alt=media&token=ec5c0836-31d5-48f4-b431-7480817f1adc\",  # noqa\n",
        "    \"QQP\": \"https://firebasestorage.googleapis.com/v0/b/mtl-sentence-representations.appspot.com/o/data%2FQQP-clean.zip?alt=media&token=11a647cb-ecd3-49c9-9d31-79f8ca8fe277\",  # noqa\n",
        "    \"STS\": \"https://firebasestorage.googleapis.com/v0/b/mtl-sentence-representations.appspot.com/o/data%2FSTS-B.zip?alt=media&token=bddb94a7-8706-4e0d-a694-1109e12273b5\",  # noqa\n",
        "    \"MNLI\": \"https://firebasestorage.googleapis.com/v0/b/mtl-sentence-representations.appspot.com/o/data%2FMNLI.zip?alt=media&token=50329ea1-e339-40e2-809c-10c40afff3ce\",  # noqa\n",
        "    \"SNLI\": \"https://firebasestorage.googleapis.com/v0/b/mtl-sentence-representations.appspot.com/o/data%2FSNLI.zip?alt=media&token=4afcfbb2-ff0c-4b2d-a09a-dbf07926f4df\",  # noqa\n",
        "    \"QNLI\": \"https://firebasestorage.googleapis.com/v0/b/mtl-sentence-representations.appspot.com/o/data%2FQNLIv2.zip?alt=media&token=6fdcf570-0fc5-4631-8456-9505272d1601\",  # noqa\n",
        "    \"RTE\": \"https://firebasestorage.googleapis.com/v0/b/mtl-sentence-representations.appspot.com/o/data%2FRTE.zip?alt=media&token=5efa7e85-a0bb-4f19-8ea2-9e1840f077fb\",  # noqa\n",
        "    \"WNLI\": \"https://firebasestorage.googleapis.com/v0/b/mtl-sentence-representations.appspot.com/o/data%2FWNLI.zip?alt=media&token=068ad0a0-ded7-4bd7-99a5-5e00222e0faf\",  # noqa\n",
        "    \"diagnostic\": [\n",
        "        \"https://storage.googleapis.com/mtl-sentence-representations.appspot.com/tsvsWithoutLabels%2FAX.tsv?GoogleAccessId=firebase-adminsdk-0khhl@mtl-sentence-representations.iam.gserviceaccount.com&Expires=2498860800&Signature=DuQ2CSPt2Yfre0C%2BiISrVYrIFaZH1Lc7hBVZDD4ZyR7fZYOMNOUGpi8QxBmTNOrNPjR3z1cggo7WXFfrgECP6FBJSsURv8Ybrue8Ypt%2FTPxbuJ0Xc2FhDi%2BarnecCBFO77RSbfuz%2Bs95hRrYhTnByqu3U%2FYZPaj3tZt5QdfpH2IUROY8LiBXoXS46LE%2FgOQc%2FKN%2BA9SoscRDYsnxHfG0IjXGwHN%2Bf88q6hOmAxeNPx6moDulUF6XMUAaXCSFU%2BnRO2RDL9CapWxj%2BDl7syNyHhB7987hZ80B%2FwFkQ3MEs8auvt5XW1%2Bd4aCU7ytgM69r8JDCwibfhZxpaa4gd50QXQ%3D%3D\",  # noqa\n",
        "        \"https://www.dropbox.com/s/ju7d95ifb072q9f/diagnostic-full.tsv?dl=1\",\n",
        "    ],\n",
        "}\n",
        "\n",
        "MRPC_TRAIN = \"https://dl.fbaipublicfiles.com/senteval/senteval_data/msr_paraphrase_train.txt\"\n",
        "MRPC_TEST = \"https://dl.fbaipublicfiles.com/senteval/senteval_data/msr_paraphrase_test.txt\"\n",
        "\n",
        "\n",
        "def download_and_extract(task, data_dir):\n",
        "    print(\"Downloading and extracting %s...\" % task)\n",
        "    data_file = \"%s.zip\" % task\n",
        "    urllib.request.urlretrieve(TASK2PATH[task], data_file)\n",
        "    with zipfile.ZipFile(data_file) as zip_ref:\n",
        "        zip_ref.extractall(data_dir)\n",
        "    os.remove(data_file)\n",
        "    print(\"\\tCompleted!\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3CVHOXQY9yVm",
        "colab_type": "code",
        "outputId": "7200e0b8-df61-4945-b496-6fd0242edb9e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "download_and_extract('MNLI', '.')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading and extracting MNLI...\n",
            "\tCompleted!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vOR0Q1Yg-HmN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers import glue_convert_examples_to_features as convert_examples_to_features\n",
        "from transformers import BertTokenizer\n",
        "from torch.utils.data import TensorDataset, RandomSampler, DataLoader, random_split\n",
        "\n",
        "processor = MnliProcessor()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yuUwBKpn-TIK",
        "colab_type": "text"
      },
      "source": [
        "#### Data loaders\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kMdQZUjO-MI7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_mnli_bert_dataloaders():\n",
        "  # ----------------------\n",
        "  # TRAIN/VAL DATALOADERS\n",
        "  # ----------------------\n",
        "  train = processor.get_train_examples('MNLI')\n",
        "  features = convert_examples_to_features(train,\n",
        "                                          tokenizer,\n",
        "                                          label_list=['contradiction','neutral','entailment'],\n",
        "                                          max_length=128,\n",
        "                                          output_mode='classification',\n",
        "                                          pad_on_left=False,\n",
        "                                          pad_token=tokenizer.pad_token_id,\n",
        "                                          pad_token_segment_id=0)\n",
        "  train_dataset = TensorDataset(torch.tensor([f.input_ids for f in features], dtype=torch.long), \n",
        "                                torch.tensor([f.attention_mask for f in features], dtype=torch.long), \n",
        "                                torch.tensor([f.token_type_ids for f in features], dtype=torch.long), \n",
        "                                torch.tensor([f.label for f in features], dtype=torch.long))\n",
        "\n",
        "  nb_train_samples = int(0.95 * len(train_dataset))\n",
        "  nb_val_samples = len(train_dataset) - nb_train_samples\n",
        "\n",
        "  bert_mnli_train_dataset, bert_mnli_val_dataset = random_split(train_dataset, [nb_train_samples, nb_val_samples])\n",
        "\n",
        "  # train loader\n",
        "  train_sampler = RandomSampler(bert_mnli_train_dataset)\n",
        "  bert_mnli_train_dataloader = DataLoader(bert_mnli_train_dataset, sampler=train_sampler, batch_size=32)\n",
        "\n",
        "  # val loader\n",
        "  val_sampler = RandomSampler(bert_mnli_val_dataset)\n",
        "  bert_mnli_val_dataloader = DataLoader(bert_mnli_val_dataset, sampler=val_sampler, batch_size=32)\n",
        "\n",
        "\n",
        "  # ----------------------\n",
        "  # TEST DATALOADERS\n",
        "  # ----------------------\n",
        "  dev = processor.get_dev_examples('MNLI')\n",
        "  features = convert_examples_to_features(dev,\n",
        "                                          tokenizer,\n",
        "                                          label_list=['contradiction','neutral','entailment'],\n",
        "                                          max_length=128,\n",
        "                                          output_mode='classification',\n",
        "                                          pad_on_left=False,\n",
        "                                          pad_token=tokenizer.pad_token_id,\n",
        "                                          pad_token_segment_id=0)\n",
        "\n",
        "  bert_mnli_test_dataset = TensorDataset(torch.tensor([f.input_ids for f in features], dtype=torch.long), \n",
        "                                torch.tensor([f.attention_mask for f in features], dtype=torch.long), \n",
        "                                torch.tensor([f.token_type_ids for f in features], dtype=torch.long), \n",
        "                                torch.tensor([f.label for f in features], dtype=torch.long))\n",
        "\n",
        "  # test dataset\n",
        "  test_sampler = RandomSampler(bert_mnli_test_dataset)\n",
        "  bert_mnli_test_dataloader = DataLoader(bert_mnli_test_dataset, sampler=test_sampler, batch_size=32)\n",
        "  \n",
        "  return bert_mnli_train_dataloader, bert_mnli_val_dataloader, bert_mnli_test_dataloader"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iV-baDhN-U6B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bert_mnli_train_dataloader, bert_mnli_val_dataloader, bert_mnli_test_dataloader = generate_mnli_bert_dataloaders()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yr7eaxkF-djf",
        "colab_type": "text"
      },
      "source": [
        "### BERT Lightning module!\n",
        "\n",
        "Finally, we can create the LightningModule"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UIXLW8CO-W8w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class BertMNLIFinetuner(pl.LightningModule):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(BertMNLIFinetuner, self).__init__()\n",
        "        \n",
        "        self.bert = bert\n",
        "        self.W = nn.Linear(bert.config.hidden_size, 3)\n",
        "        self.num_classes = 3\n",
        "\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
        "      \n",
        "        h, _, attn = self.bert(input_ids=input_ids, \n",
        "                         attention_mask=attention_mask, \n",
        "                         token_type_ids=token_type_ids)\n",
        "        \n",
        "        h_cls = h[:, 0]\n",
        "        logits = self.W(h_cls)\n",
        "        return logits, attn\n",
        "\n",
        "    def training_step(self, batch, batch_nb):\n",
        "        # batch\n",
        "        input_ids, attention_mask, token_type_ids, label = batch\n",
        "         \n",
        "        # fwd\n",
        "        y_hat, attn = self.forward(input_ids, attention_mask, token_type_ids)\n",
        "        \n",
        "        # loss\n",
        "        loss = F.cross_entropy(y_hat, label)\n",
        "        \n",
        "        # logs\n",
        "        tensorboard_logs = {'train_loss': loss}\n",
        "        return {'loss': loss, 'log': tensorboard_logs}\n",
        "\n",
        "    def validation_step(self, batch, batch_nb):\n",
        "        # batch\n",
        "        input_ids, attention_mask, token_type_ids, label = batch\n",
        "         \n",
        "        # fwd\n",
        "        y_hat, attn = self.forward(input_ids, attention_mask, token_type_ids)\n",
        "        \n",
        "        # loss\n",
        "        loss = F.cross_entropy(y_hat, label)\n",
        "        \n",
        "        # acc\n",
        "        a, y_hat = torch.max(y_hat, dim=1)\n",
        "        val_acc = accuracy_score(y_hat.cpu(), label.cpu())\n",
        "        val_acc = torch.tensor(val_acc)\n",
        "\n",
        "        return {'val_loss': loss, 'val_acc': val_acc}\n",
        "\n",
        "    def validation_end(self, outputs):\n",
        "        avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()\n",
        "        avg_val_acc = torch.stack([x['val_acc'] for x in outputs]).mean()\n",
        "\n",
        "        tensorboard_logs = {'val_loss': avg_loss, 'avg_val_acc': avg_val_acc}\n",
        "        return {'avg_val_loss': avg_loss, 'progress_bar': tensorboard_logs}\n",
        "\n",
        "    def test_step(self, batch, batch_nb):\n",
        "        input_ids, attention_mask, token_type_ids, label = batch\n",
        "        \n",
        "        y_hat, attn = self.forward(input_ids, attention_mask, token_type_ids)\n",
        "        \n",
        "        a, y_hat = torch.max(y_hat, dim=1)\n",
        "        test_acc = accuracy_score(y_hat.cpu(), label.cpu())\n",
        "        \n",
        "        return {'test_acc': torch.tensor(test_acc)}\n",
        "\n",
        "    def test_end(self, outputs):\n",
        "\n",
        "        avg_test_acc = torch.stack([x['test_acc'] for x in outputs]).mean()\n",
        "\n",
        "        tensorboard_logs = {'avg_test_acc': avg_test_acc}\n",
        "        return {'avg_test_acc': avg_test_acc, 'log': tensorboard_logs, 'progress_bar': tensorboard_logs}\n",
        "    \n",
        "    def configure_optimizers(self):\n",
        "        return torch.optim.Adam([p for p in self.parameters() if p.requires_grad], lr=2e-05, eps=1e-08)\n",
        "\n",
        "\n",
        "    @pl.data_loader\n",
        "    def train_dataloader(self):\n",
        "        return bert_mnli_train_dataloader\n",
        "\n",
        "    @pl.data_loader\n",
        "    def val_dataloader(self):\n",
        "        return bert_mnli_val_dataloader\n",
        "\n",
        "    @pl.data_loader\n",
        "    def test_dataloader(self):\n",
        "        return bert_mnli_test_dataloader"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FHt8tgwa_DcM",
        "colab_type": "text"
      },
      "source": [
        "#### Trainer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gMRMJ-Kd-oup",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bert_finetuner = BertMNLIFinetuner()\n",
        "\n",
        "# most basic trainer, uses good defaults (1 gpu)\n",
        "trainer = pl.Trainer(gpus=1)    \n",
        "trainer.fit(bert_finetuner) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ARIT37rDdIZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}